{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameensanobarsubiya/African-Vulture-Optimization/blob/main/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za8-Nr5k11fh"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Eq10uEbw0E4l"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm # For progress bars\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Assuming common defaults, adjust if needed\n",
        "pad_idx = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else 0 # Often 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoding(src)\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx).to(device)\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences\n",
        "        encoded_english = encoded_english[:self.max_length] + [pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "max_length = 128 # Define max length again for dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx) # Ignore padding in loss calculation\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding\n",
        "        for _ in range(max_length):\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token if it was added)\n",
        "        # Adjust the slicing if your tokenizer's decode handles the start token differently\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) # Exclude the initial start token\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "kQdR9AHRWQn3",
        "outputId": "02ad5d95-889c-4813-894d-1a0eb1f5500f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ff8585a34f646aea617c80bc5bbd404",
            "ed92c0c741194967b1a7573b195b1452",
            "769afa8f5d13494eb4229cf7e53c10f7",
            "56523066ff5848a696df46f2614665ee",
            "6e563386ca18484f87a11c6a812d3698",
            "46de9ca86c454cd4bca33b77fa498b87",
            "f0e33f1bc330452c96f5b34c8503d4b0",
            "c4174d9857b44102851deff37d59d4fc",
            "f4e6423ca6fa45e7a3e0ac6cebbd3dc1",
            "de746834e4a54b91938570b58dd03070",
            "78d5b3a957fb41fa8d699b48e135e9a0"
          ]
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: -1, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff8585a34f646aea617c80bc5bbd404"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-529999195>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-529999195>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-529999195>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-529999195>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHI3vyhv5p85"
      },
      "source": [
        "## **Introduction to Colab and Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVi775ZJ2bsy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    exit()\n",
        "\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "pad_idx = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "max_length = max_actual_length + 10\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.size(0) > self.pe.size(0):\n",
        "            raise RuntimeError(f\"Input sequence length ({x.size(0)}) exceeds positional encoding max_len ({self.pe.size(0)}). Increase max_length.\")\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoding(src)\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "src_vocab_size = tokenizer.get_piece_size()\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length).to(device)\n",
        "\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        target_sequence = [start_token_id]\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "fWM3itldX93_",
        "outputId": "79ff0cbc-61df-4a7f-c6a9-a1a00662b511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: -1, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting max_length for padding, truncation, and model to: 1050\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1452708037>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Assuming common defaults, adjust if needed\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset to determine appropriate max_length\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    # Add 2 for start and end tokens in target sequence\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set max_length to be slightly larger than the maximum actual length found\n",
        "# This provides some buffer for variations in sentence length during generation\n",
        "max_length = max_actual_length + 10\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # Calculate positions\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # Calculate division term for sinusoidal functions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # Apply sin to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cos to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # Reshape for broadcasting and register as buffer\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input sequence length does not exceed positional encoding max_len\n",
        "        if x.size(0) > self.pe.size(0):\n",
        "            raise RuntimeError(f\"Input sequence length ({x.size(0)}) exceeds positional encoding max_len ({self.pe.size(0)}). Increase max_length.\")\n",
        "        # Add positional encoding to the input embeddings\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        # Calculate dimension of each head\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear layers for queries, keys, and values\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        # Output linear layer\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores (Q * K^T / sqrt(d_k))\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # Apply mask if provided (fill masked positions with a large negative value)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        # Calculate weighted sum of values (Attention * V)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Apply linear transformations and split into heads\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        # Concatenate heads and apply output linear layer\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        # Two linear layers with a ReLU activation in between\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the feed-forward network\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Self-attention sub-layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Feed-forward sub-layer\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Apply self-attention with residual connection and normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        # Apply feed-forward network with residual connection and normalization\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        # Stack of encoder layers\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Apply embedding and positional encoding\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoding(src)\n",
        "        # Apply dropout\n",
        "        src = self.dropout(src)\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Masked self-attention sub-layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Cross-attention sub-layer (attention over encoder output)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        # Feed-forward sub-layer\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Apply masked self-attention with residual connection and normalization\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        # Apply cross-attention with residual connection and normalization\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        # Apply feed-forward network with residual connection and normalization\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        # Stack of decoder layers\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Output linear layer to predict vocabulary\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Apply embedding and positional encoding\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "        # Apply dropout\n",
        "        tgt = self.dropout(tgt)\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        # Apply output linear layer\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize and move the model to the selected device\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length).to(device)\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            output = model(src, tgt_input)\n",
        "            # Reshape output for loss calculation\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "I5TLcIRAYZD1",
        "outputId": "a2310e3b-2c08-44dc-ff66-e5e765f11166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting max_length for padding, truncation, and model to: 1050\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4260962786>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;31m# Initialize and move the model to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset to determine appropriate max_length\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    # Add 2 for start and end tokens in target sequence\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set max_length to be slightly larger than the maximum actual length found\n",
        "# This provides some buffer for variations in sentence length during generation\n",
        "max_length = max_actual_length + 10\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # Calculate positions\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # Calculate division term for sinusoidal functions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # Apply sin to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cos to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # Reshape for broadcasting and register as buffer\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input sequence length does not exceed positional encoding max_len\n",
        "        # Note: This check is for runtime, the buffer is created at init\n",
        "        if x.size(1) > self.pe.size(0): # Check sequence length dim (dim 1)\n",
        "            # While this check is good for runtime, the error is happening at .to(device)\n",
        "            # so the issue might be with pe itself.\n",
        "            raise RuntimeError(f\"Input sequence length ({x.size(1)}) exceeds positional encoding max_len ({self.pe.size(0)}). Increase max_length.\")\n",
        "        # Add positional encoding to the input embeddings\n",
        "        # Adjust slicing to match batch_size, seq_len, d_model\n",
        "        x = x + self.pe[:x.size(1), :].unsqueeze(0) # Add unsqueeze(0) for broadcasting with batch\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        # Calculate dimension of each head\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear layers for queries, keys, and values\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        # Output linear layer\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores (Q * K^T / sqrt(d_k))\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # Apply mask if provided (fill masked positions with a large negative value)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        # Calculate weighted sum of values (Attention * V)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Apply linear transformations and split into heads\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        # Concatenate heads and apply output linear layer\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        # Two linear layers with a ReLU activation in between\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the feed-forward network\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Self-attention sub-layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Feed-forward sub-layer\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Apply self-attention with residual connection and normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        # Apply feed-forward network with residual connection and normalization\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        # Stack of encoder layers\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Apply embedding\n",
        "        src = self.embedding(src)\n",
        "        # Apply positional encoding\n",
        "        src = self.pos_encoding(src)\n",
        "        # Apply dropout\n",
        "        src = self.dropout(src)\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Masked self-attention sub-layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Cross-attention sub-layer (attention over encoder output)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        # Feed-forward sub-layer\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Apply masked self-attention with residual connection and normalization\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        # Apply cross-attention with residual connection and normalization\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        # Apply feed-forward network with residual connection and normalization\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        # Stack of decoder layers\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Output linear layer to predict vocabulary\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Apply embedding\n",
        "        tgt = self.embedding(tgt)\n",
        "        # Apply positional encoding\n",
        "        tgt = self.pos_encoding(tgt)\n",
        "        # Apply dropout\n",
        "        tgt = self.dropout(tgt)\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        # Apply output linear layer\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length)\n",
        "\n",
        "# --- Diagnostic: Print min/max of positional encoding buffer ---\n",
        "print(\"\\nChecking positional encoding buffer:\")\n",
        "try:\n",
        "    pe_buffer = model.encoder.pos_encoding.pe\n",
        "    print(f\"Positional Encoding buffer shape: {pe_buffer.shape}\")\n",
        "    print(f\"Positional Encoding buffer min value: {pe_buffer.min().item()}\")\n",
        "    print(f\"Positional Encoding buffer max value: {pe_buffer.max().item()}\")\n",
        "except AttributeError:\n",
        "    print(\"Could not access positional encoding buffer.\")\n",
        "print(\"-\" * 20)\n",
        "# --- End Diagnostic ---\n",
        "\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "print(\"Model moved to device.\")\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "xJsTEVl5YsQ9",
        "outputId": "63219c69-993b-4396-9a06-22e267f9cd60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting max_length for padding, truncation, and model to: 1050\n",
            "\n",
            "Checking positional encoding buffer:\n",
            "Positional Encoding buffer shape: torch.Size([1050, 1, 512])\n",
            "Positional Encoding buffer min value: -1.0\n",
            "Positional Encoding buffer max value: 1.0\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1696244923>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;31m# Move the model to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model moved to device.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset to determine appropriate max_length\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    # Add 2 for start and end tokens in target sequence\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set max_length to be slightly larger than the maximum actual length found\n",
        "# This provides some buffer for variations in sentence length during generation\n",
        "max_length = max_actual_length + 10\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# Modified PositionalEncoding - now just a helper to calculate PE, not a Module buffer\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        # Calculate dimension of each head\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear layers for queries, keys, and values\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        # Output linear layer\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores (Q * K^T / sqrt(d_k))\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # Apply mask if provided (fill masked positions with a large negative value)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        # Calculate weighted sum of values (Attention * V)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Apply linear transformations and split into heads\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        # Concatenate heads and apply output linear layer\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        # Two linear layers with a ReLU activation in between\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the feed-forward network\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Self-attention sub-layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Feed-forward sub-layer\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Apply self-attention with residual connection and normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        # Apply feed-forward network with residual connection and normalization\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding is now calculated in forward\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Apply embedding\n",
        "        src = self.embedding(src)\n",
        "        # Add positional encoding\n",
        "        # Ensure sequence length does not exceed pre-calculated PE length\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :] # Slice PE to match current sequence length\n",
        "\n",
        "\n",
        "        # Apply dropout\n",
        "        src = self.dropout(src)\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Masked self-attention sub-layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Cross-attention sub-layer (attention over encoder output)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        # Feed-forward sub-layer\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        # Layer normalization and dropout\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Apply masked self-attention with residual connection and normalization\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        # Apply cross-attention with residual connection and normalization\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        # Apply feed-forward network with residual connection and normalization\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding is now calculated in forward\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Apply embedding\n",
        "        tgt = self.embedding(tgt)\n",
        "        # Add positional encoding\n",
        "        # Ensure sequence length does not exceed pre-calculated PE length\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :] # Slice PE to match current sequence length\n",
        "\n",
        "        # Apply dropout\n",
        "        tgt = self.dropout(tgt)\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        # Apply output linear layer\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        # Pass device to Encoder/Decoder so PE is calculated on the correct device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the model\n",
        "# Device is handled inside Encoder/Decoder for PE\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length)\n",
        "\n",
        "# Move the entire model to the selected device\n",
        "# This should now work more smoothly as PE is already on the correct device\n",
        "model.to(device)\n",
        "print(\"Model moved to device.\")\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "sRMVOkc2ZRJL",
        "outputId": "aeebc0c6-9b4e-4ab5-ca7d-2eddfaa446f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting max_length for padding, truncation, and model to: 1050\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2320131346>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;31m# Device is handled inside Encoder/Decoder for PE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;31m# Move the entire model to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2320131346>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_len)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Initialize encoder and decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Pass device to Encoder/Decoder so PE is calculated on the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# Store padding index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2320131346>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Pre-calculate positional encoding and keep it on the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_positional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2320131346>\u001b[0m in \u001b[0;36mget_positional_encoding\u001b[0;34m(max_len, d_model, device)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset to determine appropriate max_length\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    # Add 2 for start and end tokens in target sequence\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set max_length to be slightly larger than the maximum actual length found\n",
        "# This provides some buffer for variations in sentence length during generation\n",
        "max_length = max_actual_length + 10\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the model components\n",
        "encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_length)\n",
        "decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_length)\n",
        "\n",
        "# Move encoder and decoder individually to the device\n",
        "print(\"Attempting to move Encoder to device...\")\n",
        "encoder.to(device)\n",
        "print(\"Encoder moved to device.\")\n",
        "\n",
        "print(\"Attempting to move Decoder to device...\")\n",
        "decoder.to(device)\n",
        "print(\"Decoder moved to device.\")\n",
        "\n",
        "# Initialize the Transformer model with the moved components\n",
        "# Note: This might still fail if the issue is with the Transformer container itself\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length)\n",
        "# Replace the initially created encoder/decoder with the ones already on the device\n",
        "model.encoder = encoder\n",
        "model.decoder = decoder\n",
        "\n",
        "print(\"Transformer model assembled with components on device.\")\n",
        "\n",
        "# The model is now on the device, so no further .to(device) call here.\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "9o2Hu-ViZsIO",
        "outputId": "9447d16a-ac6d-4428-e1fc-b1100c8a2e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting max_length for padding, truncation, and model to: 1050\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-634767338>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;31m# Initialize the model components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-634767338>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# Pre-calculate positional encoding and keep it on the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_positional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-634767338>\u001b[0m in \u001b[0;36mget_positional_encoding\u001b[0;34m(max_len, d_model, device)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "    # Check if the detected CUDA version matches PyTorch's expected version\n",
        "    if torch.backends.cuda.is_built() and torch.version.cuda:\n",
        "        print(f\"PyTorch expected CUDA version: {torch.version.cuda}\")\n",
        "        # This is just an informational print, the actual compatibility depends on drivers etc.\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset to determine appropriate max_length\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    # Add 2 for start and end tokens in target sequence\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set max_length to be slightly larger than the maximum actual length found\n",
        "# This provides some buffer for variations in sentence length during generation\n",
        "max_length = max_actual_length + 10\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        # Pass device to Encoder/Decoder so PE is calculated on the correct device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the Transformer model (components will be moved individually below)\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length)\n",
        "\n",
        "# --- Attempt to move components individually for diagnosis ---\n",
        "print(\"Attempting to move Encoder to device...\")\n",
        "try:\n",
        "    model.encoder.to(device)\n",
        "    print(\"Encoder moved to device successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error moving Encoder to device: {e}\")\n",
        "    # You can add more specific checks here if needed\n",
        "\n",
        "\n",
        "print(\"Attempting to move Decoder to device...\")\n",
        "try:\n",
        "    model.decoder.to(device)\n",
        "    print(\"Decoder moved to device successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error moving Decoder to device: {e}\")\n",
        "    # You can add more specific checks here if needed\n",
        "\n",
        "# At this point, if the error occurred, the traceback should indicate whether it was\n",
        "# during the encoder or decoder transfer.\n",
        "# If both were successful, the issue might be elsewhere, but the original traceback\n",
        "# points specifically to the .to(device) on the whole model.\n",
        "# If both succeeded, the model is now effectively on the device, so we can proceed.\n",
        "\n",
        "\n",
        "print(\"Transformer model initialized and components moved to device.\")\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "iS1NRvS5aF1N",
        "outputId": "fa8e7c0b-68de-4e75-f150-9c7e6b583c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "PyTorch expected CUDA version: 12.4\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting max_length for padding, truncation, and model to: 1050\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2364795426>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;31m# Initialize the Transformer model (components will be moved individually below)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;31m# --- Attempt to move components individually for diagnosis ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2364795426>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_len)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# Initialize encoder and decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# Pass device to Encoder/Decoder so PE is calculated on the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# Store padding index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2364795426>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Pre-calculate positional encoding and keep it on the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_positional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2364795426>\u001b[0m in \u001b[0;36mget_positional_encoding\u001b[0;34m(max_len, d_model, device)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "    # Check if the detected CUDA version matches PyTorch's expected version\n",
        "    if torch.backends.cuda.is_built() and torch.version.cuda:\n",
        "        print(f\"PyTorch expected CUDA version: {torch.version.cuda}\")\n",
        "        # This is just an informational print, the actual compatibility depends on drivers etc.\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_"
      ],
      "metadata": {
        "id": "SZzH1DJPbRzU",
        "outputId": "2acdfc92-519c-4f29-98a9-743a18c3995a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "PyTorch expected CUDA version: 12.4\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'start_' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3206263056>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Set pad_idx to 0 explicitly as it's the standard for padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mpad_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mstart_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'start_' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "    # Check if the detected CUDA version matches PyTorch's expected version\n",
        "    if torch.backends.cuda.is_built() and torch.version.cuda:\n",
        "        print(f\"PyTorch expected CUDA version: {torch.version.cuda}\")\n",
        "        # This is just an informational print, the actual compatibility depends on drivers etc.\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties before moving to device ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties (before moving to device) ---\")\n",
        "    print(f\"Shape: {pe.shape}\")\n",
        "    print(f\"dtype: {pe.dtype}\")\n",
        "    print(f\"device: {pe.device}\")\n",
        "    print(f\"Min value: {pe.min().item()}\")\n",
        "    print(f\"Max value: {pe.max().item()}\")\n",
        "    # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    memory_bytes = pe.numel() * pe.element_size()\n",
        "    print(f\"Estimated CPU memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        # Pass device to Encoder/Decoder so PE is calculated on the correct device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the Transformer model\n",
        "# The positional encoding tensors will be created and moved to the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length)\n",
        "\n",
        "# The model is now on the device because PE was moved during init, and other\n",
        "# parameters are implicitly moved with the module structure.\n",
        "# Explicitly calling model.to(device) here is redundant for moving the PE,\n",
        "# but it's good practice to ensure all other parameters/buffers are on device.\n",
        "# However, since the error is at PE creation/move, this line will not be reached.\n",
        "# model.to(device) # This line is not needed for the error diagnosis\n",
        "\n",
        "\n",
        "print(\"Transformer model initialized.\")\n",
        "# If the code reaches this point, the PE tensors were moved successfully.\n",
        "# The error might then occur later during training.\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * (max_length - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:])\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=max_length, start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "TUGTuQuwbjno",
        "outputId": "507a5f3a-8a15-4797-9519-ed227acf6dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "CUDA version: 12.4\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "PyTorch expected CUDA version: 12.4\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting fixed max_length for padding, truncation, and model to: 128\n",
            "\n",
            "--- Positional Encoding Tensor Properties (before moving to device) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cpu\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated CPU memory usage: 0.25 MB\n",
            "--------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2346350321>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;31m# The positional encoding tensors will be created and moved to the device during\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;31m# the initialization of the Encoder and Decoder modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;31m# The model is now on the device because PE was moved during init, and other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2346350321>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_len)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# Initialize encoder and decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Pass device to Encoder/Decoder so PE is calculated on the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Store padding index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2346350321>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Pre-calculate positional encoding and keep it on the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_positional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2346350321>\u001b[0m in \u001b[0;36mget_positional_encoding\u001b[0;34m(max_len, d_model, device)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# --- End Diagnostic ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- Step 1: Set CUDA_LAUNCH_BLOCKING=1 for better error messages ---\n",
        "# This needs to be set before any CUDA operations are performed.\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# --- Step 2: Set device to GPU if available, else CPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 3: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    # Get details for the current device (device 0)\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "    print(\"If CUDA is not available, the rest of the code requiring GPU will not work.\")\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "# Exit if CUDA is not available, as the model is designed for GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Exiting because CUDA is not available.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    # Masks should be created on the same device as the input tensors\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    # Ensure nopeak_mask is created on the same device as tgt\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "# This function now takes device as an argument and creates the tensor directly on that device\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    # Create positional encoding tensor directly on the target device\n",
        "    pe = torch.zeros(max_len, d_model, device=device)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model), device=device)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties ---\")\n",
        "    print(f\"Shape: {pe.shape}\")\n",
        "    print(f\"dtype: {pe.dtype}\")\n",
        "    print(f\"device: {pe.device}\")\n",
        "    print(f\"Min value: {pe.min().item()}\")\n",
        "    print(f\"Max value: {pe.max().item()}\")\n",
        "    # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    memory_bytes = pe.numel() * pe.element_size()\n",
        "    print(f\"Estimated Device memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    print(\"-------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    return pe\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Ensure calculations are on the same device\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding directly on the specified device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        src = self.embedding(src) # Embedding output is on the device of the input tensor (src)\n",
        "        if src.device != self.pe.device:\n",
        "            # This should ideally not happen if inputs are moved to device, but adding a safety check\n",
        "            print(f\"Warning: Encoder input device ({src.device}) does not match PE device ({self.pe.device}). Moving PE.\")\n",
        "            self.pe = self.pe.to(src.device) # Move PE to match input device\n",
        "\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding directly on the specified device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        tgt = self.embedding(tgt) # Embedding output is on the device of the input tensor (tgt)\n",
        "        if tgt.device != self.pe.device:\n",
        "             # This should ideally not happen if inputs are moved to device, but adding a safety check\n",
        "            print(f\"Warning: Decoder input device ({tgt.device}) does not match PE device ({self.pe.device}). Moving PE.\")\n",
        "            self.pe = self.pe.to(tgt.device) # Move PE to match input device\n",
        "\n",
        "\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder, passing the target device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences (should be on the same device as src/tgt)\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# --- Initialize the Transformer model, passing the device ---\n",
        "# Positional encoding tensors will be created directly on the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "print(\"\\nInitializing Transformer model...\")\n",
        "try:\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device)\n",
        "    print(\"Transformer model initialized successfully.\")\n",
        "    # The model should largely be on the device now because its components were.\n",
        "    # A final .to(device) here ensures any remaining parameters/buffers are moved,\n",
        "    # but might not be strictly necessary if PE move was the only issue.\n",
        "    # If the error happens below, it's *not* the PE move causing the direct error anymore.\n",
        "    model.to(device)\n",
        "    print(\"Model confirmed to be on device.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Transformer model initialization or final .to(device): {e}\")\n",
        "    print(\"Please check the traceback above for the specific line that caused the error.\")\n",
        "    # If the error was at get_positional_encoding(..., device=device),\n",
        "    # the issue is likely with the CUDA environment itself.\n",
        "    exit() # Exit if model initialization fails\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * (self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "print(\"\\nStarting training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    for batch in tqdm(custom_train_dataloader, desc=\"Training\"):\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the model\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape output for loss calculation\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the validation data\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:model.encoder.max_len] + [pad_idx] * (model.encoder.max_len - len(encoded_input))\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the model's max_len for generation limit\n",
        "        for _ in range(model.decoder.max_len):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "# Use the model's max_length for the translation function as well\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=model.encoder.max_len, # Pass the model's max_length\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tXxT71tJcUYI",
        "outputId": "3ded1371-579c-4076-ad8d-56e676b65885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f919ed42c5f743ca9fa697e0c5d01f35",
            "a066317a18fa4fa2a9c05ddfbce207db",
            "b56e2adc9e8948deb3e14be6b1010952",
            "7dd8588531cf4772b2ce116e091c9223",
            "0405c27495e34b509d47c90d5582b00d",
            "4844aae30632468f8bdf929aa06d6b13",
            "557f7c1580e8400fa5076855cc16d825",
            "ae55dc256c9d4dd6a89a57ed1093c27a",
            "6eb30d7a7bce44a6a01861c14863cd43",
            "290763e167d145d28537c25393937ba8",
            "c6ac616c422f433aa73edb22b4cb60f7"
          ]
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "\n",
            "--- Simple CUDA Tensor Test ---\n",
            "Error creating/moving test tensor to cuda: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "-----------------------------\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting fixed max_length for padding, truncation, and model to: 128\n",
            "\n",
            "Initializing Transformer model...\n",
            "Error during Transformer model initialization or final .to(device): CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Please check the traceback above for the specific line that caused the error.\n",
            "\n",
            "Starting training loop...\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f919ed42c5f743ca9fa697e0c5d01f35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1124830617>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# Move batch tensors to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Target input for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- Step 1: Set CUDA_LAUNCH_BLOCKING=1 for better error messages ---\n",
        "# This needs to be set before any CUDA operations are performed.\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# --- Step 2: Set device to GPU if available, else CPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 3: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    # Get details for the current device (device 0)\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "    print(\"If CUDA is not available, the rest of the code requiring GPU will not work.\")\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "# Exit if CUDA is not available, as the model is designed for GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Exiting because CUDA is not available.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    # Masks should be created on the same device as the input tensors\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    # Ensure nopeak_mask is created on the same device as tgt\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "# This function now takes device as an argument and creates the tensor directly on that device\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    # Create positional encoding tensor directly on the target device\n",
        "    pe = torch.zeros(max_len, d_model, device=device)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model), device=device)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties ---\")\n",
        "    print(f\"Shape: {pe.shape}\")\n",
        "    print(f\"dtype: {pe.dtype}\")\n",
        "    print(f\"device: {pe.device}\")\n",
        "    print(f\"Min value: {pe.min().item()}\")\n",
        "    print(f\"Max value: {pe.max().item()}\")\n",
        "    # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    memory_bytes = pe.numel() * pe.element_size()\n",
        "    print(f\"Estimated Device memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    print(\"-------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    return pe\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Ensure calculations are on the same device\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding directly on the specified device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        src = self.embedding(src) # Embedding output is on the device of the input tensor (src)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding directly on the specified device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        tgt = self.embedding(tgt) # Embedding output is on the device of the input tensor (tgt)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "         # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder, passing the target device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences (should be on the same device as src/tgt)\n",
        "        # Ensure masks are created on the correct device.\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# --- Initialize the Transformer model, passing the device ---\n",
        "# Positional encoding tensors will be created directly on the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "print(\"\\nInitializing Transformer model...\")\n",
        "try:\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device)\n",
        "    print(\"Transformer model initialized successfully.\")\n",
        "    # The model should largely be on the device now because its components were.\n",
        "    # A final .to(device) here ensures any remaining parameters/buffers are moved,\n",
        "    # but might not be strictly necessary if PE move was the only issue.\n",
        "    # If the error happens below, it's *not* the PE move causing the direct error anymore.\n",
        "    # model.to(device) # This was causing an error before, likely due to PE not being on device first.\n",
        "                      # Now PE is created on device, so this might work, but let's rely on\n",
        "                      # PE initialization moving the core parts and inputs being moved in the loop.\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Transformer model initialization: {e}\")\n",
        "    print(\"Please check the traceback above for the specific line that caused the error.\")\n",
        "    # If the error was at get_positional_encoding(..., device=device),\n",
        "    # the issue is likely with the CUDA environment itself.\n",
        "    exit() # Exit if model initialization fails\n",
        "\n",
        "\n",
        "print(\"Transformer model should be on device after initialization.\")\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_english)) # Use max(0, ...) to avoid negative padding\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_urdu)) # Use max(0, ...) to avoid negative padding\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "print(\"\\nStarting training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    # --- Add diagnostic prints for batch tensors before moving to device ---\n",
        "    print(\"\\n--- Diagnostic: Inspecting batch tensors before moving to device ---\")\n",
        "    for i, batch in enumerate(tqdm(custom_train_dataloader, desc=\"Training\")):\n",
        "        if i == 0: # Inspect only the first batch\n",
        "            src_batch_cpu = batch['input_ids']\n",
        "            tgt_batch_cpu = batch['labels']\n",
        "\n",
        "            print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "            print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "            print(f\"Source vocab size: {src_vocab_size}\") # Check max_value vs vocab_size\n",
        "\n",
        "            print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "            print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "            print(f\"Target vocab size: {tgt_vocab_size}\") # Check max_value vs vocab_size\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:model.encoder.max_len] + [pad_idx] * max(0, model.encoder.max_len - len(encoded_input)) # Use max(0, ...)\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the model's max_len for generation limit\n",
        "        for _ in range(model.decoder.max_len):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "# Use the model's max_length for the translation function as well\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=model.encoder.max_len, # Pass the model's max_length\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "5weHTciRc3nR",
        "outputId": "71fc9659-6ab1-4320-e8b9-cbbb87da4fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "\n",
            "--- Simple CUDA Tensor Test ---\n",
            "Successfully created and moved a test tensor to cuda.\n",
            "tensor([-2.0334, -0.5138, -0.3794,  1.0689,  0.0168,  0.4950, -1.0198, -0.2708,\n",
            "         0.0856, -1.2105], device='cuda:0')\n",
            "-----------------------------\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting fixed max_length for padding, truncation, and model to: 128\n",
            "\n",
            "Initializing Transformer model...\n",
            "Error during Transformer model initialization: exp() got an unexpected keyword argument 'device'\n",
            "Please check the traceback above for the specific line that caused the error.\n",
            "Transformer model should be on device after initialization.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1174059996>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;31m# Define optimizer (Adam) with specified learning rate and parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m# Adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- Step 1: Set CUDA_LAUNCH_BLOCKING=1 for better error messages ---\n",
        "# This needs to be set before any CUDA operations are performed.\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# --- Step 2: Set device to GPU if available, else CPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 3: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    # Get details for the current device (device 0)\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        # If this fails, there is a fundamental CUDA/PyTorch installation issue.\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "    print(\"If CUDA is not available, the rest of the code requiring GPU will not work.\")\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "# Exit if CUDA is not available, as the model is designed for GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Exiting because CUDA is not available.\")\n",
        "    # In a script, use sys.exit()\n",
        "    import sys\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    import sys\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    import sys\n",
        "    sys.exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    # Masks should be created on the same device as the input tensors\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    # Ensure nopeak_mask is created on the same device as tgt\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "# Modified to calculate exp on CPU then move to device\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    # Create positional encoding tensor on CPU first\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    # Calculate div_term on CPU\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties (before move) ---\")\n",
        "    print(f\"Shape: {pe.shape}\")\n",
        "    print(f\"dtype: {pe.dtype}\")\n",
        "    print(f\"device: {pe.device}\")\n",
        "    print(f\"Min value: {pe.min().item()}\")\n",
        "    print(f\"Max value: {pe.max().item()}\")\n",
        "    # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    memory_bytes = pe.numel() * pe.element_size()\n",
        "    print(f\"Estimated CPU memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    # Now move the completed PE tensor to the target device\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Ensure calculations are on the same device\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        # This call is where the error was happening previously.\n",
        "        # get_positional_encoding now creates PE on CPU then moves it.\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        src = self.embedding(src) # Embedding output is on the device of the input tensor (src)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        # This call is where the error was happening previously.\n",
        "        # get_positional_encoding now creates PE on CPU then moves it.\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        tgt = self.embedding(tgt) # Embedding output is on the device of the input tensor (tgt)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "         # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder, passing the target device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences (should be on the same device as src/tgt)\n",
        "        # Ensure masks are created on the correct device.\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# --- Initialize the Transformer model, passing the device ---\n",
        "# Positional encoding tensors will be created directly on the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "print(\"\\nInitializing Transformer model...\")\n",
        "try:\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device)\n",
        "    print(\"Transformer model initialized successfully.\")\n",
        "    # The model should largely be on the device now because its components were.\n",
        "    # A final .to(device) here ensures any remaining parameters/buffers are moved,\n",
        "    # but might not be strictly necessary if PE move was the only issue.\n",
        "    # If the error happens below, it's *not* the PE move causing the direct error anymore.\n",
        "    # model.to(device) # Removed this redundant/problematic call\n",
        "                      # Now PE is created on device during init, so this might work, but let's rely on\n",
        "                      # PE initialization moving the core parts and inputs being moved in the loop.\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Transformer model initialization: {e}\")\n",
        "    print(\"Please check the traceback above for the specific line that caused the error.\")\n",
        "    # If the error was at get_positional_encoding(..., device=device),\n",
        "    # the issue is likely with the CUDA environment itself.\n",
        "    import sys\n",
        "    sys.exit() # Exit if model initialization fails\n",
        "\n",
        "\n",
        "print(\"Transformer model should be on device after initialization.\")\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        # Use max(0, ...) to avoid negative padding length calculation if somehow max_length < len\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "print(\"\\nStarting training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "    # --- Add diagnostic prints for batch tensors before moving to device ---\n",
        "    # Moved these checks to just before the .to(device) call inside the loop\n",
        "    # to ensure we are inspecting the exact tensors being moved when the error occurs.\n",
        "    # print(\"\\n--- Diagnostic: Inspecting batch tensors before moving to device ---\")\n",
        "\n",
        "    for i, batch in enumerate(tqdm(custom_train_dataloader, desc=\"Training\")):\n",
        "        # if i == 0: # Inspect only the first batch\n",
        "        #     src_batch_cpu = batch['input_ids']\n",
        "        #     tgt_batch_cpu = batch['labels']\n",
        "\n",
        "        #     print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "        #     print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "        #     print(f\"Source vocab size: {src_vocab_size}\") # Check max_value vs vocab_size\n",
        "\n",
        "        #     print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "        #     print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "        #     print(f\"Target vocab size: {tgt_vocab_size}\") # Check max_value vs vocab_size\n",
        "        #     print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "\n",
        "        # Move batch tensors to the selected device\n",
        "        # --- Diagnostic: Inspecting batch tensors before moving to device (within loop) ---\n",
        "        # This will print for the batch that actually triggers the error\n",
        "        if i < 5: # Print for the first few batches to see if values are consistent\n",
        "            src_batch_cpu = batch['input_ids']\n",
        "            tgt_batch_cpu = batch['labels']\n",
        "\n",
        "            print(f\"\\nBatch {i}: Inspecting tensors before moving to {device}...\")\n",
        "            print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "            print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "            print(f\"Source vocab size: {src_vocab_size}\")\n",
        "\n",
        "            print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "            print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "            print(f\"Target vocab size: {tgt_vocab_size}\")\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:model.encoder.max_len] + [pad_idx] * max(0, model.encoder.max_len - len(encoded_input)) # Use max(0, ...)\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the model's max_len for generation limit\n",
        "        for _ in range(model.decoder.max_len):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "# Use the model's max_length for the translation function as well\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=model.encoder.max_len, # Pass the model's max_length\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id"
      ],
      "metadata": {
        "id": "PDZHKvi6djX9",
        "outputId": "9f72c43e-c8ed-4e71-e479-d71e8e444692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-1209113918>, line 590)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1209113918>\"\u001b[0;36m, line \u001b[0;32m590\u001b[0m\n\u001b[0;31m    end_token_id\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys # Import sys for sys.exit()\n",
        "\n",
        "# --- Step 1: Set CUDA_LAUNCH_BLOCKING=1 for better error messages ---\n",
        "# This needs to be set before any CUDA operations are performed.\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# --- Step 2: Set device to GPU if available, else CPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 3: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    # Get details for the current device (device 0)\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        # If this fails, there is a fundamental CUDA/PyTorch installation issue.\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "    print(\"If CUDA is not available, the rest of the code requiring GPU will not work.\")\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "# Exit if CUDA is not available, as the model is designed for GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Exiting because CUDA is not available.\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sys.exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    # Masks should be created on the same device as the input tensors\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    # Ensure nopeak_mask is created on the same device as tgt\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "# Modified to calculate exp on CPU then move to device\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    # Create positional encoding tensor on CPU first\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    # Calculate div_term on CPU\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties (before move) ---\")\n",
        "    print(f\"Shape: {pe.shape}\")\n",
        "    print(f\"dtype: {pe.dtype}\")\n",
        "    print(f\"device: {pe.device}\")\n",
        "    print(f\"Min value: {pe.min().item()}\")\n",
        "    print(f\"Max value: {pe.max().item()}\")\n",
        "    # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    memory_bytes = pe.numel() * pe.element_size()\n",
        "    print(f\"Estimated CPU memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    # Now move the completed PE tensor to the target device\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Ensure calculations are on the same device\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        src = self.embedding(src) # Embedding output is on the device of the input tensor (src)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        tgt = self.embedding(tgt) # Embedding output is on the device of the input tensor (tgt)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "         # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder, passing the target device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences (should be on the same device as src/tgt)\n",
        "        # Ensure masks are created on the correct device.\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# --- Initialize the Transformer model, passing the device ---\n",
        "# Positional encoding tensors will be created directly on the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "print(\"\\nInitializing Transformer model...\")\n",
        "try:\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device)\n",
        "    print(\"Transformer model initialized successfully.\")\n",
        "    # The model should largely be on the device now because its components were.\n",
        "    # A final .to(device) here ensures any remaining parameters/buffers are moved,\n",
        "    # but might not be strictly necessary if PE move was the only issue.\n",
        "    # If the error happens below, it's *not* the PE move causing the direct error anymore.\n",
        "    # model.to(device) # Removed this redundant/problematic call\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Transformer model initialization: {e}\")\n",
        "    print(\"Please check the traceback above for the specific line that caused the error.\")\n",
        "    # If the error was at get_positional_encoding(..., device=device),\n",
        "    # the issue is likely with the CUDA environment itself.\n",
        "    sys.exit() # Exit if model initialization fails\n",
        "\n",
        "\n",
        "print(\"Transformer model should be on device after initialization.\")\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Tokenizer\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        # Use max(0, ...) to avoid negative padding length calculation if somehow max_length < len\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "print(\"\\nStarting training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "\n",
        "    for i, batch in enumerate(tqdm(custom_train_dataloader, desc=\"Training\")):\n",
        "        # Move batch tensors to the selected device\n",
        "        # --- Diagnostic: Inspecting batch tensors before moving to device ---\n",
        "        # This will print for the batch that actually triggers the error\n",
        "        if i < 5: # Print for the first few batches to see if values are consistent\n",
        "            src_batch_cpu = batch['input_ids']\n",
        "            tgt_batch_cpu = batch['labels']\n",
        "\n",
        "            print(f\"\\nBatch {i}: Inspecting tensors before moving to {device}...\")\n",
        "            print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "            print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "            print(f\"Source vocab size: {src_vocab_size}\")\n",
        "\n",
        "            print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "            print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "            print(f\"Target vocab size: {tgt_vocab_size}\")\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:model.encoder.max_len] + [pad_idx] * max(0, model.encoder.max_len - len(encoded_input)) # Use max(0, ...)\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the model's max_len for generation limit\n",
        "        for _ in range(model.decoder.max_len):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "# Use the model's max_length for the translation function as well\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=model.encoder.max_len, # Pass the model's max_length\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id, # Corrected this line\n",
        "    pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "CTAz3_4NeB4T",
        "outputId": "7d77a707-6730-4b7f-821f-9fb377b53230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d44afe798c9438aa0c86bf3c6a0382b",
            "098a73a4c6a44d3a801b0a00e08c6bd8",
            "926e075f53674b2baf222bb02d32a67c",
            "d02c708a4d844511822e1f99636fbc8a",
            "6baf15d978e54b9aa179d20c23cdeb00",
            "0b326cfd15de481a97e0f3e8b6932c7e",
            "11e2e260a435453cb1d94cbfefcb5cc6",
            "9769e6e9db734f53bfed187e39ac0154",
            "9f604ad6c9e4441f90b0ae884a078af7",
            "8fe61cdd29fd4174aefb0e4ae5ad3344",
            "392680ea9e6449a2bdc4ee22b29c1b99"
          ]
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "\n",
            "--- Simple CUDA Tensor Test ---\n",
            "Successfully created and moved a test tensor to cuda.\n",
            "tensor([-0.2243, -1.1342,  0.5697, -1.1392,  0.5348,  0.8518, -0.9881,  0.1923,\n",
            "        -0.5311, -1.0137], device='cuda:0')\n",
            "-----------------------------\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting fixed max_length for padding, truncation, and model to: 128\n",
            "\n",
            "Initializing Transformer model...\n",
            "\n",
            "--- Positional Encoding Tensor Properties (before move) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cpu\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated CPU memory usage: 0.25 MB\n",
            "-------------------------------------------------------\n",
            "\n",
            "--- Positional Encoding Tensor Properties (before move) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cpu\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated CPU memory usage: 0.25 MB\n",
            "-------------------------------------------------------\n",
            "Transformer model initialized successfully.\n",
            "Transformer model should be on device after initialization.\n",
            "\n",
            "Starting training loop...\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d44afe798c9438aa0c86bf3c6a0382b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31971\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31968\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3624954946>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3624954946>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Pass source through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Pass target and encoder output through decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3624954946>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Ensure embedding output is on the same device as PE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Embedding output is on the device of the input tensor (src)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;31m# No need to check/move PE device here if PE is already on the correct device from init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys # Import sys for sys.exit()\n",
        "\n",
        "# --- Step 1: Set CUDA_LAUNCH_BLOCKING=1 for better error messages ---\n",
        "# This needs to be set before any CUDA operations are performed.\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# --- Step 2: Set device to GPU if available, else CPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 3: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    # Get details for the current device (device 0)\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        # If this fails, there is a fundamental CUDA/PyTorch installation issue.\n",
        "        # In this case, the PE error was likely a symptom, not the root cause.\n",
        "        print(\"Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\")\n",
        "        import sys\n",
        "        sys.exit() # Exit if fundamental CUDA test fails\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "    print(\"If CUDA is not available, the rest of the code requiring GPU will not work.\")\n",
        "    import sys\n",
        "    sys.exit() # Exit if CUDA is not available\n",
        "\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sys.exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    # Masks should be created on the same device as the input tensors\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    # Ensure nopeak_mask is created on the same device as tgt\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "# Modified to calculate exp on CPU then move to device\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    # Create positional encoding tensor on CPU first\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    # Calculate div_term on CPU\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties ---\n",
        "    # Removed CPU properties print to avoid clutter, focus on device properties if successful\n",
        "    # print(\"\\n--- Positional Encoding Tensor Properties (before move) ---\")\n",
        "    # print(f\"Shape: {pe.shape}\")\n",
        "    # print(f\"dtype: {pe.dtype}\")\n",
        "    # print(f\"device: {pe.device}\")\n",
        "    # print(f\"Min value: {pe.min().item()}\")\n",
        "    # print(f\"Max value: {pe.max().item()}\")\n",
        "    # # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    # memory_bytes = pe.numel() * pe.element_size()\n",
        "    # print(f\"Estimated CPU memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    # print(\"-------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    # Now move the completed PE tensor to the target device\n",
        "    pe_on_device = pe.to(device)\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties (after move) ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties (on device) ---\")\n",
        "    print(f\"Shape: {pe_on_device.shape}\")\n",
        "    print(f\"dtype: {pe_on_device.dtype}\")\n",
        "    print(f\"device: {pe_on_device.device}\")\n",
        "    print(f\"Min value: {pe_on_device.min().item()}\")\n",
        "    print(f\"Max value: {pe_on_device.max().item()}\")\n",
        "    # Estimate memory usage on device\n",
        "    memory_bytes_device = pe_on_device.numel() * pe_on_device.element_size()\n",
        "    print(f\"Estimated Device memory usage: {memory_bytes_device / (1024*1024):.2f} MB\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "\n",
        "    return pe_on_device\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Ensure calculations are on the same device\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        src = self.embedding(src) # Embedding output is on the device of the input tensor (src)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        tgt = self.embedding(tgt) # Embedding output is on the device of the input tensor (tgt)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "         # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder, passing the target device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences (should be on the same device as src/tgt)\n",
        "        # Ensure masks are created on the correct device.\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# --- Initialize the Transformer model, passing the device ---\n",
        "# Positional encoding tensors will be created directly on the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "print(\"\\nInitializing Transformer model...\")\n",
        "try:\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device)\n",
        "    print(\"Transformer model initialized successfully.\")\n",
        "    # The model should largely be on the device now because its components were.\n",
        "    # A final .to(device) here ensures any remaining parameters/buffers are moved,\n",
        "    # but might not be strictly necessary if PE move was the only issue.\n",
        "    # If the error happens below, it's *not* the PE move causing the direct error anymore.\n",
        "    # model.to(device) # Removed this redundant/problematic call\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Transformer model initialization: {e}\")\n",
        "    print(\"Please check the traceback above for the specific line that caused the error.\")\n",
        "    sys.exit() # Exit if model initialization fails\n",
        "\n",
        "\n",
        "print(\"Transformer model should be on device after initialization.\")\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Dataset\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        # Use max(0, ...) to avoid negative padding length calculation if somehow max_length < len\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "print(\"\\nStarting training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "\n",
        "    for i, batch in enumerate(tqdm(custom_train_dataloader, desc=\"Training\")):\n",
        "        # --- Diagnostic: Inspecting batch tensors before moving to device ---\n",
        "        # This will print for the batch that actually triggers the error\n",
        "        if i < 5: # Print for the first few batches to see if values are consistent\n",
        "            src_batch_cpu = batch['input_ids']\n",
        "            tgt_batch_cpu = batch['labels']\n",
        "\n",
        "            print(f\"\\nBatch {i}: Inspecting tensors before moving to {device}...\")\n",
        "            print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "            print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "            print(f\"Source vocab size: {src_vocab_size}\")\n",
        "\n",
        "            print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "            print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "            print(f\"Target vocab size: {tgt_vocab_size}\")\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:model.encoder.max_len] + [pad_idx] * max(0, model.encoder.max_len - len(encoded_input)) # Use max(0, ...)\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the model's max_len for generation limit\n",
        "        for _ in range(model.decoder.max_len):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence after training\n",
        "sample_english_sentence = \"This is a test sentence.\"\n",
        "# Use the model's max_length for the translation function as well\n",
        "translated_urdu = custom_translate_english_to_urdu(\n",
        "    sample_english_sentence, model, tokenizer, device,\n",
        "    max_length=model.encoder.max_len, # Pass the model's max_length\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id,\n",
        "    pad_idx=pad_idx\n",
        ")\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "print(f\"Translated Urdu: {translated_urdu}\")"
      ],
      "metadata": {
        "id": "TIrezbJEemgr",
        "outputId": "89b437ce-8dab-4069-d019-4fdc81a0b214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "987c50d889ae4fc48510f6c0575f8f7e",
            "834f051db59640ed9b5608c60789fd61",
            "adf884b7d71947c29dfd14e98a461d95",
            "203270b7a85f4f76851d83fe960c180a",
            "50da8dd7e7784301989b46e0ea94c73f",
            "30e3b63d749e47cc8f67918b6ff886c4",
            "86fdb2747daf4396ae58a172c34b7c1a",
            "cf8e0e1a77bb4cf5b8c98559120ffade",
            "ff96a82bec3343c0ae385a109e320094",
            "c6715d13ee444bc98dd068aa01385d88",
            "6f3c4970e53347bf956554bd32f07b0f"
          ]
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "\n",
            "--- Simple CUDA Tensor Test ---\n",
            "Successfully created and moved a test tensor to cuda.\n",
            "tensor([ 1.0400,  0.0233, -1.2081,  1.3635,  0.3674, -0.7474,  0.1935,  0.2716,\n",
            "        -1.0764, -0.2894], device='cuda:0')\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting fixed max_length for padding, truncation, and model to: 128\n",
            "\n",
            "Initializing Transformer model...\n",
            "\n",
            "--- Positional Encoding Tensor Properties (on device) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cuda:0\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated Device memory usage: 0.25 MB\n",
            "-------------------------------------------------------\n",
            "\n",
            "--- Positional Encoding Tensor Properties (on device) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cuda:0\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated Device memory usage: 0.25 MB\n",
            "-------------------------------------------------------\n",
            "Transformer model initialized successfully.\n",
            "Transformer model should be on device after initialization.\n",
            "\n",
            "Starting training loop...\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "987c50d889ae4fc48510f6c0575f8f7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31990\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31966\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1124362621>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1124362621>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Pass source through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Pass target and encoder output through decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1124362621>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Ensure embedding output is on the same device as PE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Embedding output is on the device of the input tensor (src)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;31m# No need to check/move PE device here if PE is already on the correct device from init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys # Import sys for sys.exit()\n",
        "\n",
        "# --- Step 1: Set CUDA_LAUNCH_BLOCKING=1 for better error messages ---\n",
        "# This needs to be set before any CUDA operations are performed.\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# --- Step 2: Set device to GPU if available, else CPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 3: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    # Get details for the current device (device 0)\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    # Check if PyTorch is built with CUDA\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        # If this fails, there is a fundamental CUDA/PyTorch installation issue.\n",
        "        # In this case, the PE error was likely a symptom, not the root cause.\n",
        "        print(\"Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\")\n",
        "        sys.exit() # Exit if fundamental CUDA test fails\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Please ensure you have a CUDA-enabled GPU and necessary drivers installed.\")\n",
        "    print(\"If CUDA is not available, the rest of the code requiring GPU will not work.\")\n",
        "    sys.exit() # Exit if CUDA is not available\n",
        "\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "# --- End Environment and Compatibility Checks ---\n",
        "\n",
        "\n",
        "# 1. Data Loading\n",
        "# Assuming your dataset file is named 'parallel-corpus.xlsx'\n",
        "# And it has columns \"SENTENCES \" (English) and \"MEANING\" (Urdu)\n",
        "try:\n",
        "    df = pd.read_excel('parallel-corpus.xlsx') # Use read_excel for .xlsx files\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'parallel-corpus.xlsx' not found. Please check the file name and path.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sys.exit()\n",
        "\n",
        "# Rename columns for easier access\n",
        "df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "\n",
        "# 2. Prepare Data for Tokenizer Training and Train SentencePiece + BPE Tokenizer\n",
        "\n",
        "# Combine English and Urdu sentences into a single file\n",
        "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "        f.write(str(row['english']) + \"\\n\")\n",
        "        f.write(str(row['urdu']) + \"\\n\")\n",
        "\n",
        "print(\"\\nCorpus file created for tokenizer training.\")\n",
        "\n",
        "# Define tokenizer training parameters\n",
        "# vocab_size: The desired size of your vocabulary\n",
        "# model_prefix: Prefix for the output model files\n",
        "# model_type: 'bpe' for BPE-based SentencePiece\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=corpus.txt --model_prefix=m_bpe --vocab_size=32000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece + BPE tokenizer trained.\")\n",
        "print(\"Model files 'm_bpe.model' and 'm_bpe.vocab' created.\")\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(\"m_bpe.model\")\n",
        "\n",
        "# Get special token IDs (these might vary based on SentencePiece default)\n",
        "# Common special tokens are <unk> (unknown), <s> (start), </s> (end), <pad> (padding)\n",
        "# You can check the m_bpe.vocab file to confirm\n",
        "# Set pad_idx to 0 explicitly as it's the standard for padding\n",
        "pad_idx = 0\n",
        "start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1 # Often 1\n",
        "end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2 # Often 2\n",
        "unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3 # Often 3\n",
        "\n",
        "print(f\"\\nSpecial Token IDs: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "\n",
        "# Find the maximum actual length in the dataset (informational now, will use fixed max_length)\n",
        "max_actual_length = 0\n",
        "for index, row in df.iterrows():\n",
        "    english_text = str(row['english'])\n",
        "    urdu_text = str(row['urdu'])\n",
        "\n",
        "    encoded_english = tokenizer.encode_as_ids(english_text)\n",
        "    encoded_urdu = tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "    urdu_sequence_length = len(encoded_urdu) + 2\n",
        "\n",
        "    max_actual_length = max(max_actual_length, len(encoded_english), urdu_sequence_length)\n",
        "\n",
        "# Set a fixed, smaller max_length for testing\n",
        "max_length = 128\n",
        "print(f\"\\nMaximum tokenized sequence length in the dataset (including special tokens): {max_actual_length}\")\n",
        "print(f\"Setting fixed max_length for padding, truncation, and model to: {max_length}\")\n",
        "\n",
        "\n",
        "# 3. Implement Transformer Encoder-Decoder from Scratch\n",
        "\n",
        "# Helper function for creating masks\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    # Masks should be created on the same device as the input tensors\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    # Create a triangular mask to prevent attention to future tokens in the target sequence\n",
        "    # Ensure nopeak_mask is created on the same device as tgt\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper\n",
        "# Modified to calculate exp on CPU then move to device\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    # Create positional encoding tensor on CPU first\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    # Calculate div_term on CPU\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties ---\n",
        "    # Removed CPU properties print to avoid clutter, focus on device properties if successful\n",
        "    # print(\"\\n--- Positional Encoding Tensor Properties (before move) ---\")\n",
        "    # print(f\"Shape: {pe.shape}\")\n",
        "    # print(f\"dtype: {pe.dtype}\")\n",
        "    # print(f\"device: {pe.device}\")\n",
        "    # print(f\"Min value: {pe.min().item()}\")\n",
        "    # print(f\"Max value: {pe.max().item()}\")\n",
        "    # # Estimate memory usage (assuming float32 = 4 bytes)\n",
        "    # memory_bytes = pe.numel() * pe.element_size()\n",
        "    # print(f\"Estimated CPU memory usage: {memory_bytes / (1024*1024):.2f} MB\")\n",
        "    # print(\"-------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "    # Now move the completed PE tensor to the target device\n",
        "    pe_on_device = pe.to(device)\n",
        "\n",
        "    # --- Diagnostic: Print PE tensor properties (on device) ---\n",
        "    print(\"\\n--- Positional Encoding Tensor Properties (on device) ---\")\n",
        "    print(f\"Shape: {pe_on_device.shape}\")\n",
        "    print(f\"dtype: {pe_on_device.dtype}\")\n",
        "    print(f\"device: {pe_on_device.device}\")\n",
        "    print(f\"Min value: {pe_on_device.min().item()}\")\n",
        "    print(f\"Max value: {pe_on_device.max().item()}\")\n",
        "    # Estimate memory usage on device\n",
        "    memory_bytes_device = pe_on_device.numel() * pe_on_device.element_size()\n",
        "    print(f\"Estimated Device memory usage: {memory_bytes_device / (1024*1024):.2f} MB\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    # --- End Diagnostic ---\n",
        "\n",
        "\n",
        "    return pe_on_device\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        # Ensure calculations are on the same device\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        src = self.embedding(src) # Embedding output is on the device of the input tensor (src)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        # Pre-calculate positional encoding and keep it on the device\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        # Ensure embedding output is on the same device as PE\n",
        "        tgt = self.embedding(tgt) # Embedding output is on the device of the input tensor (tgt)\n",
        "        # No need to check/move PE device here if PE is already on the correct device from init\n",
        "\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "         # Ensure PE slicing is correct for the current batch's sequence length\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            # Layers should handle tensors on their own device\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Initialize encoder and decoder, passing the target device\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Store padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target sequences (should be on the same device as src/tgt)\n",
        "        # Ensure masks are created on the correct device.\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        # Pass source through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        # Pass target and encoder output through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "# Define model parameters\n",
        "src_vocab_size = tokenizer.get_piece_size() # Size of the vocabulary\n",
        "tgt_vocab_size = tokenizer.get_piece_size()\n",
        "d_model = 512 # Embedding dimension\n",
        "num_layers = 6 # Number of encoder/decoder layers\n",
        "num_heads = 8 # Number of attention heads\n",
        "d_ff = 2048 # Dimension of feed-forward network\n",
        "dropout = 0.1\n",
        "\n",
        "# --- Initialize the Transformer model, passing the device ---\n",
        "# Positional encoding tensors will be created directly on the device during\n",
        "# the initialization of the Encoder and Decoder modules.\n",
        "print(\"\\nInitializing Transformer model...\")\n",
        "try:\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device)\n",
        "    print(\"Transformer model initialized successfully.\")\n",
        "    # --- Explicitly move the entire model to the device ---\n",
        "    # This ensures all parameters and buffers (including embedding weights) are on the GPU.\n",
        "    print(\"Moving model to device...\")\n",
        "    model.to(device)\n",
        "    print(\"Model moved to device successfully.\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Transformer model initialization or device transfer: {e}\")\n",
        "    print(\"Please check the traceback above for the specific line that caused the error.\")\n",
        "    sys.exit() # Exit if model initialization fails\n",
        "\n",
        "\n",
        "# print(\"Transformer model should be on device after initialization.\") # This print is now redundant\n",
        "\n",
        "\n",
        "# 4. Data Loading and Preprocessing with Custom Dataset\n",
        "\n",
        "class CustomTranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # Add start and end tokens to the target sequence\n",
        "        encoded_urdu = [self.start_token_id] + encoded_urdu + [self.end_token_id]\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        # Use max(0, ...) to avoid negative padding length calculation if somehow max_length < len\n",
        "        encoded_english = encoded_english[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_english))\n",
        "        encoded_urdu = encoded_urdu[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_urdu))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_english, dtype=torch.long),\n",
        "            'labels': torch.tensor(encoded_urdu, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create custom dataset\n",
        "custom_full_dataset = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "custom_train_size = int(0.8 * len(custom_full_dataset))\n",
        "custom_val_size = len(custom_full_dataset) - custom_train_size\n",
        "custom_train_dataset, custom_val_dataset = random_split(custom_full_dataset, [custom_train_size, custom_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "custom_batch_size = 16\n",
        "custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "custom_val_dataloader = DataLoader(custom_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "\n",
        "# 5. Training Loop (with custom model and data)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss) and ignore padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "# Define optimizer (Adam) with specified learning rate and parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "num_epochs = 5 # Adjust as needed\n",
        "\n",
        "print(\"\\nStarting training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    # Iterate over the training data\n",
        "\n",
        "    for i, batch in enumerate(tqdm(custom_train_dataloader, desc=\"Training\")):\n",
        "        # --- Diagnostic: Inspecting batch tensors before moving to device ---\n",
        "        # This will print for the batch that actually triggers the error\n",
        "        # Only print for the first batch to avoid flooding output\n",
        "        if i == 0:\n",
        "            src_batch_cpu = batch['input_ids']\n",
        "            tgt_batch_cpu = batch['labels']\n",
        "\n",
        "            print(f\"\\nBatch {i}: Inspecting tensors before moving to {device}...\")\n",
        "            print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "            print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "            print(f\"Source vocab size: {src_vocab_size}\")\n",
        "\n",
        "            print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "            print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "            print(f\"Target vocab size: {tgt_vocab_size}\")\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "        # Move batch tensors to the selected device\n",
        "        src = batch['input_ids'].to(device)\n",
        "        tgt = batch['labels'].to(device) # Target input for decoder\n",
        "\n",
        "        # The target for loss calculation is the target sequence shifted by one position\n",
        "        # because the decoder predicts the next token based on the previous ones.\n",
        "        # We also remove the last token from the target input for the decoder.\n",
        "        # The labels for the loss calculation are the actual target sequence excluding the first token.\n",
        "        # This is standard for sequence-to-sequence training.\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(custom_train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Evaluation (with custom model and data)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(custom_val_dataloader, desc=\"Validation\"):\n",
        "            # Move batch tensors to the selected device\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            # Prepare target input and labels similarly to training\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(custom_val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# 7. Inference (Translation with custom model)\n",
        "\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length=128, start_token_id=1, end_token_id=2, pad_idx=0):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:model.encoder.max_len] + [pad_idx] * max(0, model.encoder.max_len - len(encoded_input)) # Use max(0, ...)\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the model's max_len for generation limit\n",
        "        for _ in range(model.decoder.max_len):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# Translate a sample sentence"
      ],
      "metadata": {
        "id": "smdowUD6fKSL",
        "outputId": "0c1bd032-ff27-44e4-d673-8fbab7c6b113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1dc1618cbc494812b44e0848cc64b9b9",
            "2918392c1e1d4f7c9adebe3d65a71d48",
            "f731459121e340cd87e698dd4bffe828",
            "836373d595a343e796e7a7dc28b3eb51",
            "df5e74d41b154937b780ebda32b94ffd",
            "b4f3b62ebf7a46a68bb7a04d3ac45381",
            "0d94168911604faab8d7c41ca0f248e8",
            "def9e7580ddc412b92372b2a0aff4422",
            "fba5c1bd1edf4f05a94b396cb3aaccb7",
            "3ff6d473ec6b4197acde919c5ca8d2ad",
            "aea134d69ce1493493164d140d7b0cc4",
            "8af0d9dc8d7e4e27b5f481a15ca8cc2b",
            "be313ca942734640916930f90fae9a34",
            "340d866c86ca4dd9b88d841321b8255c",
            "c637893299634c078545dfc2ac71a5d0",
            "cce50bbebd4448e49afaaf6b9a2f1bf2",
            "1c8401d5cf7f470aac55091e2b52c725",
            "a840f49a2e794659978177e4ceb4e780",
            "5d27b161f8a9452892f2af067ea0c0e3",
            "90e20ea0b543472f94a78ce3ded74afe",
            "69bb1b9773ed4d4abc2a9e7c55174d43",
            "7734e366880a4dff897f18859e2c1811",
            "b0a605fb22de4dee87104a6bcd172b1b",
            "302b64d1510543c7ba4a61ee08e1325b",
            "3dff26c886814bf3be3367eae71a7f62",
            "a22e928f77b84279b45ffff73bcb2c87",
            "4f44254cc374403a9f688a968ab462d4",
            "6b1a81f0f7324094bc5c385f498ff5e4",
            "be5fa683451d4788a088ada3a46f8157",
            "e2afcb8a53c5409eb81a25441993f493",
            "a78adae9c69346d6a2441c59b1e8e66a",
            "23e0d79ea58849caaee88a053e90dce3",
            "b035738cbe71490c9f53ecf9489dc7fd",
            "90cb207d701c4e2eba2175102b0e2c4a",
            "aadd5d5a786f4903878802567a67ace4",
            "8154ca3deeb44079b8a8d30495fd2f87",
            "38bba5d7e9e241c2b34d613e4dcd7072",
            "01c57aeee2064016895006c244fe40e8",
            "1007719aa0944ed8ac2055e4df514fc0",
            "600feadbcf8e4b608fd46e8e2b164f6f",
            "cf62b8ea670d435cb9a7aaf0d2560ac3",
            "cf7843b3576440d19824c86063f1db7e",
            "00256858cf2c4753a9a5ec86a8603e73",
            "41df02db652d48e89b480fcb09bd22dd",
            "6868a4502d9948c793fe6df508b6bac6",
            "32b5aeb6b7f34215824b312ca4c35db9",
            "eebf4b425ce24be09805b8027bb3624e",
            "dec0339b13884592964c9808df31168d",
            "4afb2196251c48e4853375e62f2c039a",
            "ba262c4f55344b1684d4ca85aa048b0c",
            "3b27ed5a2eae48aaaa9f6e9ffc6e0dc6",
            "93377cf1bf204017afa105884c9fc26f",
            "fce5d6ae10384bbcbc9a9daa7b2c43b1",
            "3b438e3d270f4d6ab24c2e908f9385fa",
            "1efba444f0514c2e99edebdb2db99027",
            "4ce73824499f48249dd2e2b59c4c6118",
            "fe8f153337cb4ebc8cd8be08f067c70b",
            "c28ef18d2b8d4f79b2535fce17580d84",
            "a5bfc15d177e408fbb6557aa4c6917a8",
            "82dc7a0ab4a34550ba1459abc3afd1a4",
            "bac2152886184a2d8d18b3dcef2b3f74",
            "0396ed34c5d0436688756d42835330ca",
            "61c423f1dc854d8aa12e38d74ba0b381",
            "cb064bda3cce4b5cbb32541976956753",
            "62837827b61541f98aa2f49553a5508e",
            "5003c9098ebf4bc49f116e913fba31c1",
            "fb8bfd0a459444078eaaa12f06021128",
            "103de46aa1834bb7a83c990f33e78a35",
            "a975be5699bd4ad683b674d118c0c2db",
            "ea1b4cc0b4784333ae0abf858cea80b1",
            "673d3ade7be84262acbdaa95966798c9",
            "28f7769cec3b4cb0acfeb239ce6cd317",
            "c0e772430bda49b789964e30d36e464b",
            "e1ea2cef607343419158313a955d3d97",
            "283966b971794a2382636c7c9f1cf72d",
            "af6bd29626fc4b6c8ea71d50c8edac56",
            "0a81e8e3c6ac412bb05517bf5d9808b9",
            "4f64c4fd72234c7d84d61fdf4cefe0c2",
            "528eb77bd40d452ebb3f2f236f8f3f36",
            "565ff793fa4b4a4f9934fa9b4c6c8a3e",
            "2808f9678557484380903aa0a64ab4da",
            "4cd2ba54a0a7429792d3dee6423cfc56",
            "36b45f71f4bd46b7bb3277fbd571637f",
            "b44f3149fc134fea9c23af728fd0e833",
            "bf11d4509d094743bca664c6d088a6ca",
            "df3c68b510e14fc8ac9760af119669dc",
            "9b2849ca7be44fd6823aeb95084f44e9",
            "19fca84c1da047f3818ef8b7e1e0308c",
            "2899b8631ed543748adaf835299d57fc",
            "c152e277c4fd43b3897ed90aef66072b",
            "8d8d01e3fc404b779be251ea7c6fb5d0",
            "bff1f6cba344461083bf91b1e76ff363",
            "893ef115ae5a4a79a1add7630b42ba83",
            "65372139a1d04094b526b327a6193da8",
            "d5ca6810cc8e4d22a3a2f5239c7b3444",
            "00d0730b14b846b8b5796175923bd33c",
            "391ff6f42fcd4eb4b4aeb5d89b39c214",
            "12ab5d973e4f4b0080ee3a3639ff88f6",
            "d069c07fde104fafa9572e0835cb6460",
            "ee6703279eb14dd5b18c0737f41c9431",
            "8c5f726e1ad940c9bcfff8b02669c410",
            "82ddd28e28b348a2b62d4924a1d229d8",
            "8854641c248c4e28a5ed62c507d6b357",
            "f3d92ae15ab346c8b189242076508e71",
            "e310da46172940ecb1d0df02c20057b8",
            "5d5f70edd1954d2e97eb7e4a894c9129",
            "5332cbc38b2e4697a652310d22df8105",
            "ee81a0cb0028435d918aed70e06667f6",
            "57db203fc5c74ccab8fb14952c088db9",
            "68f8ffe7f93645faadfdaae77f26eb43"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "\n",
            "--- Simple CUDA Tensor Test ---\n",
            "Successfully created and moved a test tensor to cuda.\n",
            "tensor([-1.4512, -0.4681,  1.1984, -1.2393, -0.3966,  0.7614, -0.3013,  0.6769,\n",
            "        -0.3374,  0.8470], device='cuda:0')\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "                                          SENTENCES   \\\n",
            "0             How can I communicate with my parents?   \n",
            "1                           How can I make friends?’   \n",
            "2                              Why do I get so sad?’   \n",
            "3  If you’ve asked yourself such questions, you’r...   \n",
            "4  Depending on where you’ve turned for guidance,...   \n",
            "\n",
            "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
            "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
            "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
            "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
            "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
            "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
            "\n",
            "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
            "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
            "\n",
            "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
            "0          NaN          NaN         NaN          NaN         NaN  \n",
            "1          NaN          NaN         NaN          NaN         NaN  \n",
            "2          NaN          NaN         NaN          NaN         NaN  \n",
            "3          NaN          NaN         NaN          NaN         NaN  \n",
            "4          NaN          NaN         NaN          NaN         NaN  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Corpus file created for tokenizer training.\n",
            "SentencePiece + BPE tokenizer trained.\n",
            "Model files 'm_bpe.model' and 'm_bpe.vocab' created.\n",
            "\n",
            "Special Token IDs: Padding: 0, Start: 1, End: 2, Unknown: 0\n",
            "\n",
            "Maximum tokenized sequence length in the dataset (including special tokens): 1040\n",
            "Setting fixed max_length for padding, truncation, and model to: 128\n",
            "\n",
            "Initializing Transformer model...\n",
            "\n",
            "--- Positional Encoding Tensor Properties (on device) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cuda:0\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated Device memory usage: 0.25 MB\n",
            "-------------------------------------------------------\n",
            "\n",
            "--- Positional Encoding Tensor Properties (on device) ---\n",
            "Shape: torch.Size([1, 128, 512])\n",
            "dtype: torch.float32\n",
            "device: cuda:0\n",
            "Min value: -1.0\n",
            "Max value: 1.0\n",
            "Estimated Device memory usage: 0.25 MB\n",
            "-------------------------------------------------------\n",
            "Transformer model initialized successfully.\n",
            "Moving model to device...\n",
            "Model moved to device successfully.\n",
            "\n",
            "Starting training loop...\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dc1618cbc494812b44e0848cc64b9b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31992\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31968\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n",
            "Training Loss: 5.2224\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation:   0%|          | 0/378 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8af0d9dc8d7e4e27b5f481a15ca8cc2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 4.4681\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0a605fb22de4dee87104a6bcd172b1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31979\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31979\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n",
            "Training Loss: 4.0322\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation:   0%|          | 0/378 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90cb207d701c4e2eba2175102b0e2c4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.9376\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6868a4502d9948c793fe6df508b6bac6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31937\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31924\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n",
            "Training Loss: 3.5234\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation:   0%|          | 0/378 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ce73824499f48249dd2e2b59c4c6118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.6286\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb8bfd0a459444078eaaa12f06021128"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31953\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31924\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n",
            "Training Loss: 3.1235\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation:   0%|          | 0/378 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f64c4fd72234c7d84d61fdf4cefe0c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.4340\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2899b8631ed543748adaf835299d57fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 0: Inspecting tensors before moving to cuda...\n",
            "Source batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Source batch min value: 0, max value: 31974\n",
            "Source vocab size: 32000\n",
            "Target batch shape: torch.Size([16, 128]), dtype: torch.int64, device: cpu\n",
            "Target batch min value: 0, max value: 31965\n",
            "Target vocab size: 32000\n",
            "-----------------------------------------------------------------\n",
            "Training Loss: 2.8033\n",
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation:   0%|          | 0/378 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee6703279eb14dd5b18c0737f41c9431"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 3.3058\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# Assuming you have already defined your Transformer model classes (Encoder, Decoder, Transformer, etc.)\n",
        "# and the get_positional_encoding function, and have trained your 'model' object.\n",
        "# Assuming device, tokenizer, pad_idx, start_token_id, end_token_id, max_length,\n",
        "# src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout,\n",
        "# and the original dataframe `df` are already defined from your training code.\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Assuming df, tokenizer, pad_idx, start_token_id, end_token_id, max_length\n",
        "# and model (trained) are already defined and available in the environment\n",
        "# from your previous training steps.\n",
        "# If running in a new session, you would need to load the model here.\n",
        "# See the \"Example of Loading the trained model state_dict\" section below.\n",
        "\n",
        "\n",
        "# Re-define the translate function needed for evaluation and inference\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        # Use the model's max_length for inference consistency\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input)) # Use max(0, ...)\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the max_length for generation limit\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "\n",
        "# Assuming your validation dataset `custom_val_dataset` or a separate test set is available.\n",
        "# If you used random_split on the full dataframe, you might need access to the original dataframe `df`\n",
        "# and the indices of the test/validation split to get the original text.\n",
        "# For simplicity, let's assume `df` and the indices used for the *last* split (your validation set\n",
        "# from the training code) are available. We'll use the original validation set as our test set for demonstration.\n",
        "# In a real project, always use a test set that was *not* used for training or validation.\n",
        "\n",
        "# --- Recreate the validation dataset and splits if necessary ---\n",
        "# This block is for demonstrating in a new session. If you run this right after training,\n",
        "# the variables custom_full_dataset, custom_train_dataset, custom_val_dataset, df, tokenizer,\n",
        "# max_length, pad_idx, start_token_id, end_token_id should be available.\n",
        "# If you are running in a new session, make sure df, tokenizer, max_length, pad_idx,\n",
        "# start_token_id, end_token_id are defined first (e.g., by running the initial data loading and tokenizer parts).\n",
        "\n",
        "try:\n",
        "    # Attempt to access the existing custom_full_dataset and splits\n",
        "    print(\"\\nAttempting to use existing dataset splits...\")\n",
        "    # This block will only work if you run this code in the same session after training\n",
        "    _ = custom_full_dataset # Check if it exists\n",
        "    _ = custom_train_dataset # Check if train split exists\n",
        "    _ = custom_val_dataset # Check if val split exists\n",
        "    print(\"Existing dataset splits found.\")\n",
        "\n",
        "    # Use the validation dataset as the test set for BLEU\n",
        "    test_dataset_for_bleu = custom_val_dataset\n",
        "    print(f\"Using validation set (size {len(test_dataset_for_bleu)}) as test set for BLEU evaluation.\")\n",
        "\n",
        "except NameError:\n",
        "    # If dataset objects are not found (e.g., new session), recreate them and the splits\n",
        "    print(\"\\nExisting dataset splits not found. Recreating datasets and splits...\")\n",
        "    # Assuming df, tokenizer, max_length, start_token_id, end_token_id, pad_idx are defined\n",
        "    try:\n",
        "        custom_full_dataset_recreated = CustomTranslationDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "        custom_train_size_recreated = int(0.8 * len(custom_full_dataset_recreated))\n",
        "        custom_val_size_recreated = len(custom_full_dataset_recreated) - custom_train_size_recreated\n",
        "        custom_train_dataset_recreated, custom_val_dataset_recreated = random_split(custom_full_dataset_recreated, [custom_train_size_recreated, custom_val_size_recreated])\n",
        "\n",
        "        # Use the recreated validation dataset as the test set for BLEU\n",
        "        test_dataset_for_bleu = custom_val_dataset_recreated\n",
        "        print(f\"Recreated datasets. Using validation set (size {len(test_dataset_for_bleu)}) as test set for BLEU evaluation.\")\n",
        "    except Exception as e:\n",
        "         print(f\"An unexpected error occurred while recreating dataset: {e}\")\n",
        "         test_dataset_for_bleu = None # Ensure test_dataset_for_bleu is defined\n",
        "\n",
        "\n",
        "# --- Model Saving ---\n",
        "\n",
        "# 8. Save the trained model\n",
        "model_save_path = \"transformer_translation_model.pth\"\n",
        "print(f\"\\nSaving model state_dict to {model_save_path}...\")\n",
        "try:\n",
        "    # Save only the state dictionary (recommended)\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(\"Model state_dict saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model state_dict: {e}\")\n",
        "\n",
        "# --- Example of Loading the trained model state_dict (if needed in a new session) ---\n",
        "# Note: You would typically load this in a separate script or cell for inference/evaluation\n",
        "# if you were not continuing directly after training.\n",
        "# If you are running this immediately after training, the 'model' object is already available.\n",
        "# This section is commented out but shows how to load.\n",
        "\n",
        "# print(f\"\\nExample: Loading model state_dict from {model_save_path"
      ],
      "metadata": {
        "id": "Rpnt_Ostt6gv",
        "outputId": "3c403ecc-f0e0-4bc3-d6c4-097749645757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Attempting to use existing dataset splits...\n",
            "Existing dataset splits found.\n",
            "Using validation set (size 6033) as test set for BLEU evaluation.\n",
            "\n",
            "Saving model state_dict to transformer_translation_model.pth...\n",
            "Model state_dict saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# No need for optim, math, pandas, spm, Dataset, DataLoader, random_split, tqdm, os, sys for just inference\n",
        "\n",
        "# Assuming device, tokenizer, pad_idx, start_token_id, end_token_id, max_length\n",
        "# and model (trained and on the correct device) are already defined and available\n",
        "# in the environment from your previous training or loading steps.\n",
        "\n",
        "# Set device to GPU if available, else CPU (redundant if already set, but safe)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Re-define the translate function if not already in the current cell\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input))\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the max_length for generation limit\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# --- Example Translation ---\n",
        "\n",
        "# Define a sample English sentence to translate\n",
        "sample_english_sentence = \"This is a test sentence for translation.\"\n",
        "# Make sure your model is on the correct device and in evaluation mode\n",
        "model.to(device) # Ensure model is on device\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "# Perform the translation\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "\n",
        "try:\n",
        "    translated_urdu = custom_translate_english_to_urdu(\n",
        "        sample_english_sentence,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        device,\n",
        "        max_length=max_length, # Use the max_length consistent with your training\n",
        "        start_token_id=start_token_id,\n",
        "        end_token_id=end_token_id,\n",
        "        pad_idx=pad_idx\n",
        "    )\n",
        "    print(f\"Translated Urdu: {translated_urdu}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during translation: {e}\")\n",
        "\n",
        "print(\"\\nTranslation example complete.\")\n",
        "\n",
        "# To translate other sentences, just change the `sample_english_sentence` variable\n",
        "# and re-run the translation block."
      ],
      "metadata": {
        "id": "aSgbaanruYWZ",
        "outputId": "bf03d521-e0a1-482e-b20d-cc5b2661559d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Original English: This is a test sentence for translation.\n",
            "Translated Urdu: یہ امتحان کے لیے ایک ٹیسٹ ہے۔\n",
            "\n",
            "Translation example complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# No need for nn, optim, math, os, sys for just evaluation if model is already loaded\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split # Needed for splitting test set\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Assuming device, tokenizer, pad_idx, start_token_id, end_token_id, max_length\n",
        "# and model (trained and on the correct device) are already defined and available\n",
        "# in the environment from your previous training or loading steps.\n",
        "# Assuming the original dataframe `df` is also available.\n",
        "\n",
        "# Set device to GPU if available, else CPU (redundant if already set, but safe)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device for evaluation: {device}\")\n",
        "\n",
        "\n",
        "# Re-define the translate function if not already in the current cell\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input))\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the max_length for generation limit\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "\n",
        "# --- Prepare a Test Set for BLEU Evaluation ---\n",
        "# For demonstration, we'll split the original dataframe into train/test indices.\n",
        "# In a real scenario, you MUST use a completely separate test set that was NOT\n",
        "# used for training or validation splits.\n",
        "\n",
        "# Create a dummy Dataset to use random_split on the dataframe indices\n",
        "class IndexDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    def __getitem__(self, idx):\n",
        "         # Just return the index\n",
        "        return idx\n",
        "\n",
        "full_index_dataset = IndexDataset(df)\n",
        "\n",
        "# Split indices for train and test. This is just for demonstration.\n",
        "# Using 80/20 split for train/test indices\n",
        "train_size = int(0.8 * len(full_index_dataset))\n",
        "test_size_for_bleu = len(full_index_dataset) - train_size\n",
        "\n",
        "if test_size_for_bleu > 0:\n",
        "    # Split indices, not the actual data. Use a fixed seed for reproducibility if desired.\n",
        "    # torch.manual_seed(42) # Optional: uncomment for reproducible split\n",
        "    train_indices, test_indices_for_bleu = random_split(full_index_dataset, [train_size, test_size_for_bleu])\n",
        "    print(f\"\\nUsing {len(test_indices_for_bleu)} samples from the original dataframe as test set for BLEU evaluation.\")\n",
        "else:\n",
        "     print(\"\\nNot enough data to create a test set for BLEU evaluation.\")\n",
        "     test_indices_for_bleu = []\n",
        "\n",
        "\n",
        "# --- Calculate BLEU Score ---\n",
        "if len(test_indices_for_bleu) > 0:\n",
        "    print(\"\\nCalculating Corpus BLEU score on the test set...\")\n",
        "    # Ensure the model is on the correct device and in evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    hypotheses = [] # Model's translated token lists\n",
        "    references = [] # Actual target token lists (list of lists)\n",
        "\n",
        "    print(\"Generating translations for BLEU evaluation...\")\n",
        "    # Iterate through the test set indices to get original text and generate translations\n",
        "    for original_idx_in_df in tqdm(test_indices_for_bleu, desc=\"Translating test set\"):\n",
        "        english_text = str(df.iloc[original_idx_in_df]['english']) # Get original English text from dataframe\n",
        "        original_urdu_text = str(df.iloc[original_idx_in_df]['urdu']) # Get original Urdu text (reference)\n",
        "\n",
        "        # Generate translation using the custom_translate function\n",
        "        translated_urdu = custom_translate_english_to_urdu(\n",
        "            english_text, model, tokenizer, device,\n",
        "            max_length=max_length, # Use the max_length consistent with your training\n",
        "            start_token_id=start_token_id,\n",
        "            end_token_id=end_token_id,\n",
        "            pad_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        # Tokenize both the hypothesis and reference using the SentencePiece tokenizer\n",
        "        # NLTK's corpus_bleu expects lists of tokens (strings)\n",
        "        hypothesis_tokens = tokenizer.encode_as_pieces(translated_urdu)\n",
        "        # References need to be a list of lists, even if you only have one reference per sentence\n",
        "        reference_tokens = [tokenizer.encode_as_pieces(original_urdu_text)] # List containing one reference token list\n",
        "\n",
        "        hypotheses.append(hypothesis_tokens)\n",
        "        references.append(reference_tokens)\n",
        "\n",
        "    # Calculate corpus BLEU score\n",
        "    # Ensure hypotheses and references are not empty lists and have the same length\n",
        "    if hypotheses and references and len(hypotheses) == len(references):\n",
        "        try:\n",
        "            # NLTK's corpus_bleu takes a list of reference sentences (each a list of token strings)\n",
        "            # and a list of hypothesis sentences (each a list of token strings).\n",
        "            # Our 'references' list structure matches what corpus_bleu expects directly.\n",
        "            bleu_score = corpus_bleu(references, hypotheses)\n",
        "            print(f\"\\nCorpus BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error calculating BLEU score: {e}\")\n",
        "             print(\"This might happen if there are empty sequences or other issues with tokenization.\")\n",
        "    else:\n",
        "        print(\"\\nCould not calculate BLEU score: Hypotheses or references list is empty or their lengths mismatch.\")\n",
        "        print(f\"Number of hypotheses: {len(hypotheses)}\")\n",
        "        print(f\"Number of references: {len(references)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo test dataset samples available for BLEU evaluation.\")\n",
        "\n",
        "\n",
        "print(\"\\nBLEU evaluation complete.\")"
      ],
      "metadata": {
        "id": "hJSoCAO8vnge",
        "outputId": "de9a27d4-a5a1-4d85-9328-7756764156b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "ce4843d59d764efa90573c9a1f5fe82b",
            "2693647163ec445a8812d5cff3d9092e",
            "fd7fa77e42b2436ab46a3eb4f349f38e",
            "cdb06675daa04f2abef586e4a54849a7",
            "78fa98d39c754818aac9f8948e757da1",
            "614d3a0007ff417fbb334f53f0ef44d5",
            "9d74bf6f64c74a68827fb0d7b4aa3b42",
            "4b1546d9d5df42a6b7ceb262ca2cf999",
            "e8d2bcbd24464f9da55d62f681600e9f",
            "9c771cf57a6545258437cd1048bb0386",
            "b7ae8f42c64d44709bff4ea67df0fc86"
          ]
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device for evaluation: cuda\n",
            "\n",
            "Using 6033 samples from the original dataframe as test set for BLEU evaluation.\n",
            "\n",
            "Calculating Corpus BLEU score on the test set...\n",
            "Generating translations for BLEU evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Translating test set:   0%|          | 0/6033 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce4843d59d764efa90573c9a1f5fe82b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU Score: 0.2362\n",
            "\n",
            "BLEU evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# No need for nn, optim, math, os, sys for just inference\n",
        "\n",
        "# Assuming device, tokenizer, pad_idx, start_token_id, end_token_id, max_length\n",
        "# and model (trained and on the correct device) are already defined and available\n",
        "# in the environment from your previous training or loading steps.\n",
        "# Assuming the original dataframe `df` is also available (though not strictly\n",
        "# needed for just a single example translation).\n",
        "\n",
        "# Set device to GPU if available, else CPU (redundant if already set, but safe)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device for translation: {device}\")\n",
        "\n",
        "\n",
        "# Re-define the translate function if not already in the current cell\n",
        "def custom_translate_english_to_urdu(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence\n",
        "        encoded_input = tokenizer.encode_as_ids(text)\n",
        "        # Pad or truncate the encoded input\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input))\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token\n",
        "        target_sequence = [start_token_id]\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop - use the max_length for generation limit\n",
        "        for _ in range(max_length):\n",
        "            # Get model output for the current target sequence\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (excluding the start token)\n",
        "        # Ensure we handle the case where only the start token was generated\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) if len(target_sequence) > 1 else \"\"\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "\n",
        "# --- Single Example Translation ---\n",
        "\n",
        "print(\"\\n--- Single Example Translation ---\")\n",
        "# Ensure the model is on the correct device and in evaluation mode\n",
        "model.to(device) # Ensure model is on device\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "# Define a sample English sentence to translate\n",
        "# You can change this sentence to test different inputs\n",
        "sample_english_sentence = \"Who is your father?\"\n",
        "print(f\"Original English: {sample_english_sentence}\")\n",
        "\n",
        "try:\n",
        "    translated_urdu = custom_translate_english_to_urdu(\n",
        "        sample_english_sentence,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        device,\n",
        "        max_length=max_length, # Use the max_length consistent with your training\n",
        "        start_token_id=start_token_id,\n",
        "        end_token_id=end_token_id,\n",
        "        pad_idx=pad_idx\n",
        "    )\n",
        "    print(f\"Translated Urdu: {translated_urdu}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during translation: {e}\")\n",
        "\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# To translate other sentences, just change the `sample_english_sentence` variable\n",
        "# and re-run this block. You can add more print statements and function calls\n",
        "# to translate multiple sentences if needed."
      ],
      "metadata": {
        "id": "lDE5mzlg5b_Q",
        "outputId": "341f8d78-83bb-4436-d113-ea3cbbe8fab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device for translation: cuda\n",
            "\n",
            "--- Single Example Translation ---\n",
            "Original English: Who is your father?\n",
            "Translated Urdu: تمہارا پروردگار ہے؟\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# --- Step 0: Configuration ---\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define paths and names\n",
        "tokenizer_model_path = \"m_bpe.model\" # Path to your SentencePiece model file (same for both directions)\n",
        "data_file = 'parallel-corpus.xlsx' # Path to your Excel data file\n",
        "urdu_to_english_model_save_path = \"transformer_urdu_to_english_model.pth\" # New path for U->E model\n",
        "\n",
        "# Model Hyperparameters (MUST match the English->Urdu model architecture if using the same Transformer class)\n",
        "# These are needed to load the model correctly.\n",
        "# vocab_size will be the same for both source and target as we use one joint tokenizer\n",
        "d_model = 512\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "max_length = 128 # Use the same max_length as English->Urdu\n",
        "\n",
        "# Training Parameters\n",
        "num_epochs = 5 # Adjust as needed\n",
        "custom_batch_size = 16 # Adjust as needed\n",
        "\n",
        "# --- Step 1: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        print(\"Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\")\n",
        "        sys.exit()\n",
        "else:\n",
        "    print(\"CUDA is not available. Exiting.\")\n",
        "    sys.exit()\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "\n",
        "# --- Step 2: Load Data ---\n",
        "try:\n",
        "    df = pd.read_excel(data_file)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {data_file} not found.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "# --- Step 3: Load Tokenizer ---\n",
        "# Assuming the tokenizer model 'm_bpe.model' and 'm_bpe.vocab' were already trained\n",
        "# using the combined English and Urdu corpus in your previous run.\n",
        "try:\n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.load(tokenizer_model_path)\n",
        "    print(f\"Tokenizer loaded successfully from {tokenizer_model_path}.\")\n",
        "    pad_idx = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else 0\n",
        "    start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1\n",
        "    end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2\n",
        "    unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3\n",
        "    print(f\"Special Token IDs from tokenizer: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "    vocab_size = tokenizer.get_piece_size() # Joint vocabulary size\n",
        "    print(f\"Joint Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Tokenizer model file {tokenizer_model_path} not found.\")\n",
        "    print(\"Please train the tokenizer first by running the initial part of your English->Urdu script.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading tokenizer: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Set source and target vocab sizes - they are the same for a joint tokenizer\n",
        "src_vocab_size = vocab_size\n",
        "tgt_vocab_size = vocab_size\n",
        "\n",
        "\n",
        "# --- Step 4: Implement Transformer Model and Data Handling (Adapt for U->E) ---\n",
        "\n",
        "# Helper function for creating masks (remains the same)\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper (remains the same)\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    pe_on_device = pe.to(device)\n",
        "    # print(\"\\n--- Positional Encoding Tensor Properties (on device) ---\")\n",
        "    # print(f\"Shape: {pe_on_device.shape}, dtype: {pe_on_device.dtype}, device: {pe_on_device.device}\")\n",
        "    # print(f\"Min value: {pe_on_device.min().item()}, Max value: {pe_on_device.max().item()}\")\n",
        "    # print(f\"Estimated Device memory usage: {pe_on_device.numel() * pe_on_device.element_size() / (1024*1024):.2f} MB\")\n",
        "    # print(\"-------------------------------------------------------\")\n",
        "    return pe_on_device\n",
        "\n",
        "# MultiHeadAttention (remains the same)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "# FeedForward (remains the same)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# EncoderLayer (remains the same)\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "# Encoder (remains the same, but processes Urdu tokens)\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "# DecoderLayer (remains the same)\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "# Decoder (remains the same, but generates English tokens)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "# Transformer (remains the same architecture, handles U->E data)\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Encoder processes source language (Urdu)\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Decoder generates target language (English)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.pad_idx = pad_idx\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "# Custom Dataset for Urdu -> English\n",
        "class UrduToEnglishDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # For Urdu->English, Urdu is input, English is labels\n",
        "        encoded_input = encoded_urdu\n",
        "        encoded_labels = [self.start_token_id] + encoded_english + [self.end_token_id] # Add start/end to target English\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_input = encoded_input[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_input))\n",
        "        encoded_labels = encoded_labels[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_labels))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_input, dtype=torch.long), # Urdu tokens\n",
        "            'labels': torch.tensor(encoded_labels, dtype=torch.long)  # English tokens with start/end\n",
        "        }\n",
        "\n",
        "\n",
        "# Inference function for Urdu -> English (Greedy)\n",
        "def custom_translate_urdu_to_english(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence (Urdu)\n",
        "        encoded_input = tokenizer.encode_as_ids(text) # Input is Urdu text\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input))\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token (for English)\n",
        "        target_sequence = [start_token_id] # Start token for English generation\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            output = model(src_tensor, tgt_tensor) # src is Urdu, tgt_tensor is generated English\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (English)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) # Decode generated English tokens\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# --- Step 5: Create and Split Dataset (Urdu -> English) ---\n",
        "print(\"\\nCreating Urdu->English Dataset...\")\n",
        "urdu_to_english_full_dataset = UrduToEnglishDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "urdu_to_english_train_size = int(0.8 * len(urdu_to_english_full_dataset))\n",
        "urdu_to_english_val_size = len(urdu_to_english_full_dataset) - urdu_to_english_train_size\n",
        "urdu_to_english_train_dataset, urdu_to_english_val_dataset = random_split(urdu_to_english_full_dataset, [urdu_to_english_train_size, urdu_to_english_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "urdu_to_english_train_dataloader = DataLoader(urdu_to_english_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "urdu_to_english_val_dataloader = DataLoader(urdu_to_english_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "print(f\"Urdu->English Train dataset size: {len(urdu_to_english_train_dataset)}\")\n",
        "print(f\"Urdu->English Validation dataset size: {len(urdu_to_english_val_dataset)}\")\n",
        "\n",
        "\n",
        "# --- Step 6: Initialize and Train Urdu->English Model ---\n",
        "print(\"\\nInitializing Urdu->English Transformer model...\")\n",
        "# Create a NEW model instance for Urdu->English\n",
        "urdu_to_english_model = Transformer(\n",
        "    src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device\n",
        ")\n",
        "print(\"Urdu->English Transformer model initialized successfully.\")\n",
        "\n",
        "# Move the new model to the device\n",
        "print(\"Moving Urdu->English model to device...\")\n",
        "urdu_to_english_model.to(device)\n",
        "print(\"Urdu->English model moved to device successfully.\")\n",
        "\n",
        "\n",
        "# Define loss function and optimizer for U->E model\n",
        "urdu_to_english_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "urdu_to_english_optimizer = optim.Adam(urdu_to_english_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "print(\"\\nStarting Urdu->English training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    urdu_to_english_model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nUrdu->English Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for i, batch in enumerate(tqdm(urdu_to_english_train_dataloader, desc=\"Urdu->English Training\")):\n",
        "        src = batch['input_ids'].to(device) # Urdu input\n",
        "        tgt = batch['labels'].to(device)    # English labels\n",
        "\n",
        "        tgt_input = tgt[:, :-1] # English input for decoder\n",
        "        labels = tgt[:, 1:].contiguous().view(-1) # English labels for loss\n",
        "\n",
        "        urdu_to_english_optimizer.zero_grad()\n",
        "        output = urdu_to_english_model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = urdu_to_english_criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        urdu_to_english_optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(urdu_to_english_train_dataloader)\n",
        "    print(f\"Urdu->English Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    urdu_to_english_model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Urdu->English Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(urdu_to_english_val_dataloader, desc=\"Urdu->English Validation\"):\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = urdu_to_english_model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = urdu_to_english_criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(urdu_to_english_val_dataloader)\n",
        "    print(f\"Urdu->English Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nUrdu->English Training complete!\")\n",
        "\n",
        "\n",
        "# --- Step 7: Save the Trained Urdu->English Model ---\n",
        "print(f\"\\nSaving Urdu->English model state_dict to {urdu_to_english_model_save_path}...\")\n",
        "try:\n",
        "    torch.save(urdu_to_english_model.state_dict(), urdu_to_english_model_save_path)\n",
        "    print(\"Urdu->English model state_dict saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving Urdu->English model state_dict: {e}\")\n",
        "\n",
        "\n",
        "# --- Step 8: Prepare Test Set for Urdu->English BLEU Evaluation ---\n",
        "# Use the validation set indices from the U->E split as the test set for BLEU\n",
        "# In a real scenario, use a completely separate test set indices.\n",
        "urdu_to_english_test_indices_for_bleu = urdu_to_english_val_dataset.indices if hasattr(urdu_to_english_val_dataset, 'indices') else range(len(urdu_to_english_val_dataset))\n",
        "\n",
        "if len(urdu_to_english_test_indices_for_bleu) > 0:\n",
        "    print(f\"\\nUsing {len(urdu_to_english_test_indices_for_bleu)} samples from the original dataframe indices (from U->E validation split) as test set for BLEU evaluation.\")\n",
        "else:\n",
        "     print(\"\\nNot enough data to create a test set for Urdu->English BLEU evaluation.\")\n",
        "\n",
        "\n",
        "# --- Step 9: Calculate Urdu->English BLEU Score ---\n",
        "if len(urdu_to_english_test_indices_for_bleu) > 0:\n",
        "    print(\"\\nCalculating Corpus BLEU score on the Urdu->English test set...\")\n",
        "    urdu_to_english_model.eval() # Ensure the trained model is in evaluation mode\n",
        "\n",
        "    hypotheses = [] # Model's translated token lists (English)\n",
        "    references = [] # Actual target token lists (list of lists) (English)\n",
        "\n",
        "    print(\"Generating translations for Urdu->English BLEU evaluation...\")\n",
        "    for original_idx_in_df in tqdm(urdu_to_english_test_indices_for_bleu, desc=\"Translating U->E test set\"):\n",
        "        original_urdu_text = str(df.iloc[original_idx_in_df]['urdu']) # Get original Urdu text from dataframe\n",
        "        original_english_text = str(df.iloc[original_idx_in_df]['english']) # Get original English text (reference)\n",
        "\n",
        "        # Generate translation using the custom_translate_urdu_to_english function\n",
        "        translated_english = custom_translate_urdu_to_english(\n",
        "            original_urdu_text, urdu_to_english_model, tokenizer, device,\n",
        "            max_length=max_length,\n",
        "            start_token_id=start_token_id, # Start token for English\n",
        "            end_token_id=end_token_id,     # End token for English\n",
        "            pad_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        # Tokenize both the hypothesis (translated English) and reference (original English)\n",
        "        hypothesis_tokens = tokenizer.encode_as_pieces(translated_english)\n",
        "        reference_tokens = [tokenizer.encode_as_pieces(original_english_text)] # Reference is original English\n",
        "\n",
        "        hypotheses.append(hypothesis_tokens)\n",
        "        references.append(reference_tokens)\n",
        "\n",
        "    # Calculate corpus BLEU score\n",
        "    if hypotheses and references and len(hypotheses) == len(references):\n",
        "        try:\n",
        "            bleu_score_urdu_to_english = corpus_bleu(references, hypotheses)\n",
        "            print(f\"\\nCorpus BLEU Score (Urdu->English): {bleu_score_urdu_to_english:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error calculating Urdu->English BLEU score: {e}\")\n",
        "             print(\"This might happen if there are empty sequences or other issues with tokenization.\")\n",
        "    else:\n",
        "        print(\"\\nCould not calculate Urdu->English BLEU score: Hypotheses or references list is empty or their lengths mismatch.\")\n",
        "        print(f\"Number of hypotheses: {len(hypotheses)}\")\n",
        "        print(f\"Number of references: {len(references)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo test dataset samples available for Urdu->English BLEU evaluation.\")\n",
        "\n",
        "print(\"\\nUrdu->English BLEU evaluation complete.\")"
      ],
      "metadata": {
        "id": "ypWZgiMs88xe",
        "outputId": "c4893d18-6848-4d97-9c88-b1f4a00b18a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c3fb88ce58c74875b8333583eb24b002",
            "ead40265b53e4313bfc45a04eae0a812",
            "ba0f2eb7e4914a55a8aa215c4998834a",
            "08c3259fb2644d8890f8d7d91d5bda46",
            "57298a8c070148cfbd487e7cef8740ca",
            "2f33749d6ea44494b923bd8c73c528ec",
            "753fe44ddb8f45fb814a7596c883baca",
            "cab4ec0635d94910b71097abb8c7ea22",
            "60c5a44fd5364a0fa5448bfa35a86800",
            "d23a2c942f9e4af58f613bf810e6b3d5",
            "762b7fefd5164ed7b84e7d96969aa6a7"
          ]
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "Successfully created and moved a test tensor to cuda.\n",
            "--------------------------------------------\n",
            "Dataset loaded successfully.\n",
            "Tokenizer loaded successfully from m_bpe.model.\n",
            "Special Token IDs from tokenizer: Padding: -1, Start: 1, End: 2, Unknown: 0\n",
            "Joint Vocabulary Size: 32000\n",
            "\n",
            "Creating Urdu->English Dataset...\n",
            "Urdu->English Train dataset size: 24131\n",
            "Urdu->English Validation dataset size: 6033\n",
            "\n",
            "Initializing Urdu->English Transformer model...\n",
            "Urdu->English Transformer model initialized successfully.\n",
            "Moving Urdu->English model to device...\n",
            "Urdu->English model moved to device successfully.\n",
            "\n",
            "Starting Urdu->English training loop...\n",
            "\n",
            "Urdu->English Epoch 1/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Urdu->English Training:   0%|          | 0/1509 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3fb88ce58c74875b8333583eb24b002"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-136496067>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0murdu_to_english_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murdu_to_english_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-136496067>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-136496067>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_positional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m              \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# --- Step 0: Configuration ---\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define paths and names\n",
        "tokenizer_model_path = \"m_bpe.model\" # Path to your SentencePiece model file (same for both directions)\n",
        "data_file = 'parallel-corpus.xlsx' # Path to your Excel data file\n",
        "urdu_to_english_model_save_path = \"transformer_urdu_to_english_model.pth\" # New path for U->E model\n",
        "\n",
        "# Model Hyperparameters (MUST match the English->Urdu model architecture if using the same Transformer class)\n",
        "# These are needed to load the model correctly.\n",
        "# vocab_size will be the same for both source and target as we use one joint tokenizer\n",
        "d_model = 512\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "max_length = 128 # Use the same max_length as English->Urdu\n",
        "\n",
        "# Training Parameters\n",
        "num_epochs = 5 # Adjust as needed\n",
        "custom_batch_size = 16 # Adjust as needed\n",
        "\n",
        "# --- Step 1: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        print(\"Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\")\n",
        "        sys.exit()\n",
        "else:\n",
        "    print(\"CUDA is not available. Exiting.\")\n",
        "    sys.exit()\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "\n",
        "# --- Step 2: Load Data ---\n",
        "try:\n",
        "    df = pd.read_excel(data_file)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {data_file} not found.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "# --- Step 3: Load Tokenizer ---\n",
        "# Assuming the tokenizer model 'm_bpe.model' and 'm_bpe.vocab' were already trained\n",
        "# using the combined English and Urdu corpus in your previous run.\n",
        "try:\n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.load(tokenizer_model_path)\n",
        "    print(f\"Tokenizer loaded successfully from {tokenizer_model_path}.\")\n",
        "    pad_idx = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else 0\n",
        "    start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1\n",
        "    end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2\n",
        "    unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3\n",
        "    print(f\"Special Token IDs from tokenizer: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "    vocab_size = tokenizer.get_piece_size() # Joint vocabulary size\n",
        "    print(f\"Joint Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Tokenizer model file {tokenizer_model_path} not found.\")\n",
        "    print(\"Please train the tokenizer first by running the initial part of your English->Urdu script.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading tokenizer: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Set source and target vocab sizes - they are the same for a joint tokenizer\n",
        "src_vocab_size = vocab_size\n",
        "tgt_vocab_size = vocab_size\n",
        "\n",
        "\n",
        "# --- Step 4: Implement Transformer Model and Data Handling (Adapt for U->E) ---\n",
        "\n",
        "# Helper function for creating masks (remains the same)\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper (remains the same)\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    pe_on_device = pe.to(device)\n",
        "    # print(\"\\n--- Positional Encoding Tensor Properties (on device) ---\")\n",
        "    # print(f\"Shape: {pe_on_device.shape}, dtype: {pe_on_device.dtype}, device: {pe_on_device.device}\")\n",
        "    # print(f\"Min value: {pe_on_device.min().item()}, Max value: {pe_on_device.max().item()}\")\n",
        "    # print(f\"Estimated Device memory usage: {pe_on_device.numel() * pe_on_device.element_size() / (1024*1024):.2f} MB\")\n",
        "    # print(\"-------------------------------------------------------\")\n",
        "    return pe_on_device\n",
        "\n",
        "# MultiHeadAttention (remains the same)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "# FeedForward (remains the same)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# EncoderLayer (remains the same)\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "# Encoder (remains the same, but processes Urdu tokens)\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "# DecoderLayer (remains the same)\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "# Decoder (remains the same, but generates English tokens)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "# Transformer (remains the same architecture, handles U->E data)\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Encoder processes source language (Urdu)\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Decoder generates target language (English)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.pad_idx = pad_idx\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "# Custom Dataset for Urdu -> English\n",
        "class UrduToEnglishDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # For Urdu->English, Urdu is input, English is labels\n",
        "        encoded_input = encoded_urdu\n",
        "        encoded_labels = [self.start_token_id] + encoded_english + [self.end_token_id] # Add start/end to target English\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_input = encoded_input[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_input))\n",
        "        encoded_labels = encoded_labels[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_labels))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_input, dtype=torch.long), # Urdu tokens\n",
        "            'labels': torch.tensor(encoded_labels, dtype=torch.long)  # English tokens with start/end\n",
        "        }\n",
        "\n",
        "\n",
        "# Inference function for Urdu -> English (Greedy)\n",
        "def custom_translate_urdu_to_english(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence (Urdu)\n",
        "        encoded_input = tokenizer.encode_as_ids(text) # Input is Urdu text\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input))\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token (for English)\n",
        "        target_sequence = [start_token_id] # Start token for English generation\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            output = model(src_tensor, tgt_tensor) # src is Urdu, tgt_tensor is generated English\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (English)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) # Decode generated English tokens\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# --- Step 5: Create and Split Dataset (Urdu -> English) ---\n",
        "print(\"\\nCreating Urdu->English Dataset...\")\n",
        "urdu_to_english_full_dataset = UrduToEnglishDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "urdu_to_english_train_size = int(0.8 * len(urdu_to_english_full_dataset))\n",
        "urdu_to_english_val_size = len(urdu_to_english_full_dataset) - urdu_to_english_train_size\n",
        "urdu_to_english_train_dataset, urdu_to_english_val_dataset = random_split(urdu_to_english_full_dataset, [urdu_to_english_train_size, urdu_to_english_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "urdu_to_english_train_dataloader = DataLoader(urdu_to_english_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "urdu_to_english_val_dataloader = DataLoader(urdu_to_english_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "print(f\"Urdu->English Train dataset size: {len(urdu_to_english_train_dataset)}\")\n",
        "print(f\"Urdu->English Validation dataset size: {len(urdu_to_english_val_dataset)}\")\n",
        "\n",
        "\n",
        "# --- Step 6: Initialize and Train Urdu->English Model ---\n",
        "print(\"\\nInitializing Urdu->English Transformer model...\")\n",
        "# Create a NEW model instance for Urdu->English\n",
        "urdu_to_english_model = Transformer(\n",
        "    src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device\n",
        ")\n",
        "print(\"Urdu->English Transformer model initialized successfully.\")\n",
        "\n",
        "# Move the new model to the device\n",
        "print(\"Moving Urdu->English model to device...\")\n",
        "urdu_to_english_model.to(device)\n",
        "print(\"Urdu->English model moved to device successfully.\")\n",
        "\n",
        "\n",
        "# Define loss function and optimizer for U->E model\n",
        "urdu_to_english_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "urdu_to_english_optimizer = optim.Adam(urdu_to_english_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "print(\"\\nStarting Urdu->English training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    urdu_to_english_model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nUrdu->English Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for i, batch in enumerate(tqdm(urdu_to_english_train_dataloader, desc=\"Urdu->English Training\")):\n",
        "\n",
        "        # --- Diagnostic: Inspecting batch tensors before moving to device (within loop) ---\n",
        "        # Print for the first batch of each epoch\n",
        "        if i == 0:\n",
        "            src_batch_cpu = batch['input_ids']\n",
        "            tgt_batch_cpu = batch['labels']\n",
        "\n",
        "            print(f\"\\nBatch {i}: Inspecting U->E tensors before moving to {device}...\")\n",
        "            print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "            print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "            print(f\"Source vocab size (Urdu input): {src_vocab_size}\") # src is Urdu for this model\n",
        "\n",
        "            print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "            print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "            print(f\"Target vocab size (English labels): {tgt_vocab_size}\") # tgt is English for this model\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "\n",
        "        src = batch['input_ids'].to(device) # Urdu input\n",
        "        tgt = batch['labels'].to(device)    # English labels\n",
        "\n",
        "        tgt_input = tgt[:, :-1] # English input for decoder\n",
        "        labels = tgt[:, 1:].contiguous().view(-1) # English labels for loss\n",
        "\n",
        "        urdu_to_english_optimizer.zero_grad()\n",
        "        output = urdu_to_english_model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = urdu_to_english_criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        urdu_to_english_optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(urdu_to_english_train_dataloader)\n",
        "    print(f\"Urdu->English Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    urdu_to_english_model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Urdu->English Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(urdu_to_english_val_dataloader, desc=\"Urdu->English Validation\")):\n",
        "             # --- Diagnostic: Inspecting batch tensors before moving to device (within loop) ---\n",
        "            # Print for the first batch of evaluation\n",
        "            if i == 0 and epoch == 0: # Only print once per training run\n",
        "                src_batch_cpu = batch['input_ids']\n",
        "                tgt_batch_cpu = batch['labels']\n",
        "\n",
        "                print(f\"\\nBatch {i}: Inspecting U->E Val tensors before moving to {device}...\")\n",
        "                print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "                print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "                print(f\"Source vocab size (Urdu input): {src_vocab_size}\")\n",
        "\n",
        "                print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "                print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "                print(f\"Target vocab size (English labels): {tgt_vocab_size}\")\n",
        "                print(\"-----------------------------------------------------------------\")\n",
        "            # --- End diagnostic prints ---\n",
        "\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = urdu_to_english_model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = urdu_to_english_criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(urdu_to_english_val_dataloader)\n",
        "    print(f\"Urdu->English Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nUrdu->English Training complete!\")\n",
        "\n",
        "\n",
        "# --- Step 7: Save the Trained Urdu->English Model ---\n",
        "print(f\"\\nSaving Urdu->English model state_dict to {urdu_to_english_model_save_path}...\")\n",
        "try:\n",
        "    torch.save(urdu_to_english_model.state_dict(), urdu_to_english_model_save_path)\n",
        "    print(\"Urdu->English model state_dict saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving Urdu->English model state_dict: {e}\")\n",
        "\n",
        "\n",
        "# --- Step 8: Prepare Test Set for Urdu->English BLEU Evaluation ---\n",
        "# Use the validation set indices from the U->E split as the test set for BLEU\n",
        "# In a real scenario, use a completely separate test set indices.\n",
        "urdu_to_english_test_indices_for_bleu = urdu_to_english_val_dataset.indices if hasattr(urdu_to_english_val_dataset, 'indices') else range(len(urdu_to_english_val_dataset))\n",
        "\n",
        "if len(urdu_to_english_test_indices_for_bleu) > 0:\n",
        "    print(f\"\\nUsing {len(urdu_to_english_test_indices_for_bleu)} samples from the original dataframe indices (from U->E validation split) as test set for BLEU evaluation.\")\n",
        "else:\n",
        "     print(\"\\nNot enough data to create a test set for Urdu->English BLEU evaluation.\")\n",
        "\n",
        "\n",
        "# --- Step 9: Calculate Urdu->English BLEU Score ---\n",
        "if len(urdu_to_english_test_indices_for_bleu) > 0:\n",
        "    print(\"\\nCalculating Corpus BLEU score on the Urdu->English test set...\")\n",
        "    urdu_to_english_model.eval() # Ensure the trained model is in evaluation mode\n",
        "\n",
        "    hypotheses = [] # Model's translated token lists (English)\n",
        "    references = [] # Actual target token lists (list of lists) (English)\n",
        "\n",
        "    print(\"Generating translations for Urdu->English BLEU evaluation...\")\n",
        "    for original_idx_in_df in tqdm(urdu_to_english_test_indices_for_bleu, desc=\"Translating U->E test set\"):\n",
        "        original_urdu_text = str(df.iloc[original_idx_in_df]['urdu']) # Get original Urdu text from dataframe\n",
        "        original_english_text = str(df.iloc[original_idx_in_df]['english']) # Get original English text (reference)\n",
        "\n",
        "        # Generate translation using the custom_translate_urdu_to_english function\n",
        "        translated_english = custom_translate_urdu_to_english(\n",
        "            original_urdu_text, urdu_to_english_model, tokenizer, device,\n",
        "            max_length=max_length,\n",
        "            start_token_id=start_token_id, # Start token for English\n",
        "            end_token_id=end_token_id,     # End token for English\n",
        "            pad_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        # Tokenize both the hypothesis (translated English) and reference (original English)\n",
        "        hypothesis_tokens = tokenizer.encode_as_pieces(translated_english)\n",
        "        reference_tokens = [tokenizer.encode_as_pieces(original_english_text)] # Reference is original English\n",
        "\n",
        "        hypotheses.append(hypothesis_tokens)\n",
        "        references.append(reference_tokens)\n",
        "\n",
        "    # Calculate corpus BLEU score\n",
        "    if hypotheses and references and len(hypotheses) == len(references):\n",
        "        try:\n",
        "            bleu_score_urdu_to_english = corpus_bleu(references, hypotheses)\n",
        "            print(f\"\\nCorpus BLEU Score (Urdu->English): {bleu_score_urdu_to_english:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error calculating Urdu->English BLEU score: {e}\")\n",
        "             print(\"This might happen if there are empty sequences or other issues with tokenization.\")\n",
        "    else:\n",
        "        print(\"\\nCould not calculate Urdu->English BLEU score: Hypotheses or references list is empty or their lengths mismatch.\")\n",
        "        print(f\"Number of hypotheses: {len(hypotheses)}\")\n",
        "        print(f\"Number of references: {len(references)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo test dataset samples available for Urdu->English BLEU evaluation.\")\n",
        "\n",
        "print(\"\\nUrdu->English BLEU evaluation complete.\")"
      ],
      "metadata": {
        "id": "utVjgu77-BRp",
        "outputId": "50e750a5-9f7d-4654-a6fa-21e10993d9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "Error creating/moving test tensor to cuda: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-13-4141178837>\", line 55, in <cell line: 0>\n",
            "    test_tensor = torch.randn(10).to(device)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-13-4141178837>\", line 60, in <cell line: 0>\n",
            "    sys.exit()\n",
            "SystemExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4141178837>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Successfully created and moved a test tensor to {device}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-4141178837>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import sys\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# --- Step 0: Configuration ---\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for better error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "print(\"CUDA_LAUNCH_BLOCKING is set to 1\")\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define paths and names\n",
        "tokenizer_model_path = \"m_bpe.model\" # Path to your SentencePiece model file (same for both directions)\n",
        "data_file = 'parallel-corpus.xlsx' # Path to your Excel data file\n",
        "urdu_to_english_model_save_path = \"transformer_urdu_to_english_model.pth\" # New path for U->E model\n",
        "\n",
        "# Model Hyperparameters (MUST match the English->Urdu model architecture if using the same Transformer class)\n",
        "# These are needed to load the model correctly.\n",
        "# vocab_size will be the same for both source and target as we use one joint tokenizer\n",
        "d_model = 512\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "max_length = 128 # Use the same max_length as English->Urdu\n",
        "\n",
        "# Training Parameters\n",
        "num_epochs = 5 # Adjust as needed\n",
        "custom_batch_size = 16 # Adjust as needed\n",
        "\n",
        "# --- Step 1: Environment and Compatibility Checks ---\n",
        "print(\"\\n--- Environment and Compatibility Checks ---\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    current_device = torch.cuda.current_device()\n",
        "    print(f\"Current CUDA device: {current_device}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")\n",
        "    print(f\"GPU Capability: {torch.cuda.get_device_capability(current_device)}\")\n",
        "    print(f\"PyTorch built with CUDA: {torch.backends.cuda.is_built()}\")\n",
        "\n",
        "    # --- Simple CUDA Tensor Test ---\n",
        "    print(\"\\n--- Simple CUDA Tensor Test ---\")\n",
        "    try:\n",
        "        # Attempt to create and move a small tensor to the GPU\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(f\"Successfully created and moved a test tensor to {device}.\")\n",
        "        print(test_tensor)\n",
        "    except Exception as e:\n",
        "        # If this fails, there is a fundamental CUDA/PyTorch installation issue.\n",
        "        print(f\"Error creating/moving test tensor to {device}: {e}\")\n",
        "        print(\"Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\")\n",
        "        # We exit here because training requiring CUDA will fail anyway\n",
        "        sys.exit()\n",
        "else:\n",
        "    print(\"CUDA is not available. Exiting as GPU is required for this model.\")\n",
        "    sys.exit()\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "\n",
        "# --- Step 2: Load Data ---\n",
        "try:\n",
        "    df = pd.read_excel(data_file)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    df.rename(columns={\"SENTENCES \": \"english\", \"MEANING\": \"urdu\"}, inplace=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {data_file} not found.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "# --- Step 3: Load Tokenizer ---\n",
        "# Assuming the tokenizer model 'm_bpe.model' and 'm_bpe.vocab' were already trained\n",
        "# using the combined English and Urdu corpus in your previous run.\n",
        "try:\n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.load(tokenizer_model_path)\n",
        "    print(f\"Tokenizer loaded successfully from {tokenizer_model_path}.\")\n",
        "    pad_idx = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else 0\n",
        "    start_token_id = tokenizer.bos_id() if hasattr(tokenizer, 'bos_id') else 1\n",
        "    end_token_id = tokenizer.eos_id() if hasattr(tokenizer, 'eos_id') else 2\n",
        "    unk_token_id = tokenizer.unk_id() if hasattr(tokenizer, 'unk_id') else 3\n",
        "    print(f\"Special Token IDs from tokenizer: Padding: {pad_idx}, Start: {start_token_id}, End: {end_token_id}, Unknown: {unk_token_id}\")\n",
        "    vocab_size = tokenizer.get_piece_size() # Joint vocabulary size\n",
        "    print(f\"Joint Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Tokenizer model file {tokenizer_model_path} not found.\")\n",
        "    print(\"Please train the tokenizer first by running the initial part of your English->Urdu script.\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading tokenizer: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Set source and target vocab sizes - they are the same for a joint tokenizer\n",
        "src_vocab_size = vocab_size\n",
        "tgt_vocab_size = vocab_size\n",
        "\n",
        "\n",
        "# --- Step 4: Implement Transformer Model and Data Handling (Adapt for U->E) ---\n",
        "\n",
        "# Helper function for creating masks (remains the same)\n",
        "def create_masks(src, tgt, pad_idx):\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.shape[1]\n",
        "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length, device=tgt.device), diagonal=1).type(torch.bool) == False).unsqueeze(0).unsqueeze(0)\n",
        "    tgt_mask = tgt_mask & nopeak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# PositionalEncoding helper (remains the same)\n",
        "def get_positional_encoding(max_len, d_model, device):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term_cpu = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term_cpu)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term_cpu)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    pe_on_device = pe.to(device)\n",
        "    # print(\"\\n--- Positional Encoding Tensor Properties (on device) ---\")\n",
        "    # print(f\"Shape: {pe_on_device.shape}, dtype: {pe_on_device.dtype}, device: {pe_on_device.device}\")\n",
        "    # print(f\"Min value: {pe_on_device.min().item()}, Max value: {pe_on_device.max().item()}\")\n",
        "    # print(f\"Estimated Device memory usage: {pe_on_device.numel() * pe_on_device.element_size() / (1024*1024):.2f} MB\")\n",
        "    # print(\"-------------------------------------------------------\")\n",
        "    return pe_on_device\n",
        "\n",
        "# MultiHeadAttention (remains the same)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, v)\n",
        "        return output\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "# FeedForward (remains the same)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# EncoderLayer (remains the same)\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))\n",
        "        return x\n",
        "\n",
        "# Encoder (remains the same, but processes Urdu tokens)\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "    def forward(self, src, mask):\n",
        "        src = self.embedding(src)\n",
        "        if src.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({src.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        src = src + self.pe[:, :src.size(1), :]\n",
        "        src = self.dropout(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return src\n",
        "\n",
        "# DecoderLayer (remains the same)\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
        "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))\n",
        "        ffn_output = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout3(ffn_output))\n",
        "        return tgt\n",
        "\n",
        "# Decoder (remains the same, but generates English tokens)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, max_len=5000, device=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.pe = get_positional_encoding(max_len, d_model, device)\n",
        "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
        "        tgt = self.embedding(tgt)\n",
        "        if tgt.size(1) > self.max_len:\n",
        "             raise RuntimeError(f\"Input sequence length ({tgt.size(1)}) exceeds positional encoding max_len ({self.max_len}). Increase max_length.\")\n",
        "        tgt = tgt + self.pe[:, :tgt.size(1), :]\n",
        "        tgt = self.dropout(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "# Transformer (remains the same architecture, handles U->E data)\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1, pad_idx=0, max_len=5000, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Encoder processes source language (Urdu)\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        # Decoder generates target language (English)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len=max_len, device=device)\n",
        "        self.pad_idx = pad_idx\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = create_masks(src, tgt, self.pad_idx)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "# Custom Dataset for Urdu -> English\n",
        "class UrduToEnglishDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length, start_token_id, end_token_id, pad_idx):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_text = str(self.dataframe.iloc[idx]['english'])\n",
        "        urdu_text = str(self.dataframe.iloc[idx]['urdu'])\n",
        "\n",
        "        # Encode sentences using the custom tokenizer\n",
        "        encoded_english = self.tokenizer.encode_as_ids(english_text)\n",
        "        encoded_urdu = self.tokenizer.encode_as_ids(urdu_text)\n",
        "\n",
        "        # For Urdu->English, Urdu is input, English is labels\n",
        "        encoded_input = encoded_urdu\n",
        "        encoded_labels = [self.start_token_id] + encoded_english + [self.end_token_id] # Add start/end to target English\n",
        "\n",
        "        # Pad or truncate sequences to the defined max_length\n",
        "        encoded_input = encoded_input[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_input))\n",
        "        encoded_labels = encoded_labels[:self.max_length] + [self.pad_idx] * max(0, self.max_length - len(encoded_labels))\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoded_input, dtype=torch.long), # Urdu tokens\n",
        "            'labels': torch.tensor(encoded_labels, dtype=torch.long)  # English tokens with start/end\n",
        "        }\n",
        "\n",
        "\n",
        "# Inference function for Urdu -> English (Greedy)\n",
        "def custom_translate_urdu_to_english(text, model, tokenizer, device, max_length, start_token_id, end_token_id, pad_idx):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        # Encode the source sentence (Urdu)\n",
        "        encoded_input = tokenizer.encode_as_ids(text) # Input is Urdu text\n",
        "        encoded_input = encoded_input[:max_length] + [pad_idx] * max(0, max_length - len(encoded_input))\n",
        "\n",
        "        # Convert to tensor, add batch dimension, and move to device\n",
        "        src_tensor = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Initialize the target sequence with the start token (for English)\n",
        "        target_sequence = [start_token_id] # Start token for English generation\n",
        "        tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Greedy decoding loop\n",
        "        for _ in range(max_length):\n",
        "            output = model(src_tensor, tgt_tensor) # src is Urdu, tgt_tensor is generated English\n",
        "            # Get logits for the last token in the target sequence\n",
        "            last_token_logits = output[:, -1, :]\n",
        "            # Get the predicted token ID with the highest probability\n",
        "            _, predicted_token_id = torch.max(last_token_logits, dim=-1)\n",
        "\n",
        "            # If the predicted token is the end token or padding, stop generation\n",
        "            if predicted_token_id.item() == end_token_id or predicted_token_id.item() == pad_idx:\n",
        "                break\n",
        "\n",
        "            # Append the predicted token to the target sequence\n",
        "            target_sequence.append(predicted_token_id.item())\n",
        "            # Update the target tensor with the new sequence\n",
        "            tgt_tensor = torch.tensor(target_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the generated sequence (English)\n",
        "        translated_text = tokenizer.decode_ids(target_sequence[1:]) # Decode generated English tokens\n",
        "\n",
        "        return translated_text\n",
        "\n",
        "# --- Step 5: Create and Split Dataset (Urdu -> English) ---\n",
        "print(\"\\nCreating Urdu->English Dataset...\")\n",
        "urdu_to_english_full_dataset = UrduToEnglishDataset(df, tokenizer, max_length, start_token_id, end_token_id, pad_idx)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "urdu_to_english_train_size = int(0.8 * len(urdu_to_english_full_dataset))\n",
        "urdu_to_english_val_size = len(urdu_to_english_full_dataset) - urdu_to_english_train_size\n",
        "urdu_to_english_train_dataset, urdu_to_english_val_dataset = random_split(urdu_to_english_full_dataset, [urdu_to_english_train_size, urdu_to_english_val_size])\n",
        "\n",
        "# Define batch size and create data loaders\n",
        "urdu_to_english_train_dataloader = DataLoader(urdu_to_english_train_dataset, batch_size=custom_batch_size, shuffle=True)\n",
        "urdu_to_english_val_dataloader = DataLoader(urdu_to_english_val_dataset, batch_size=custom_batch_size)\n",
        "\n",
        "print(f\"Urdu->English Train dataset size: {len(urdu_to_english_train_dataset)}\")\n",
        "print(f\"Urdu->English Validation dataset size: {len(urdu_to_english_val_dataset)}\")\n",
        "\n",
        "\n",
        "# --- Step 6: Initialize and Train Urdu->English Model ---\n",
        "print(\"\\nInitializing Urdu->English Transformer model...\")\n",
        "# Create a NEW model instance for Urdu->English\n",
        "urdu_to_english_model = Transformer(\n",
        "    src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, pad_idx, max_length, device=device\n",
        ")\n",
        "print(\"Urdu->English Transformer model initialized successfully.\")\n",
        "\n",
        "# Move the new model to the device\n",
        "print(\"Moving Urdu->English model to device...\")\n",
        "urdu_to_english_model.to(device)\n",
        "print(\"Urdu->English model moved to device successfully.\")\n",
        "\n",
        "\n",
        "# Define loss function and optimizer for U->E model\n",
        "urdu_to_english_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "urdu_to_english_optimizer = optim.Adam(urdu_to_english_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "print(\"\\nStarting Urdu->English training loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    urdu_to_english_model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"\\nUrdu->English Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for i, batch in enumerate(tqdm(urdu_to_english_train_dataloader, desc=\"Urdu->English Training\")):\n",
        "\n",
        "        # --- Diagnostic: Inspecting batch tensors before moving to device (within loop) ---\n",
        "        # Print for the first batch of each epoch\n",
        "        if i == 0: # Only print for the very first batch overall (i==0 and epoch==0)\n",
        "             if epoch == 0:\n",
        "                src_batch_cpu = batch['input_ids']\n",
        "                tgt_batch_cpu = batch['labels']\n",
        "\n",
        "                print(f\"\\nBatch {i}: Inspecting U->E tensors before moving to {device}...\")\n",
        "                print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "                print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "                print(f\"Source vocab size (Urdu input): {src_vocab_size}\") # src is Urdu for this model\n",
        "\n",
        "                print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "                print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "                print(f\"Target vocab size (English labels): {tgt_vocab_size}\") # tgt is English for this model\n",
        "                print(\"-----------------------------------------------------------------\")\n",
        "        # --- End diagnostic prints ---\n",
        "\n",
        "\n",
        "        src = batch['input_ids'].to(device) # Urdu input\n",
        "        tgt = batch['labels'].to(device)    # English labels\n",
        "\n",
        "        tgt_input = tgt[:, :-1] # English input for decoder\n",
        "        labels = tgt[:, 1:].contiguous().view(-1) # English labels for loss\n",
        "\n",
        "        urdu_to_english_optimizer.zero_grad()\n",
        "        output = urdu_to_english_model(src, tgt_input)\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "        loss = urdu_to_english_criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        urdu_to_english_optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(urdu_to_english_train_dataloader)\n",
        "    print(f\"Urdu->English Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    urdu_to_english_model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    print(\"Urdu->English Evaluating...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(urdu_to_english_val_dataloader, desc=\"Urdu->English Validation\")):\n",
        "             # --- Diagnostic: Inspecting batch tensors before moving to device (within loop) ---\n",
        "            # Print for the first batch of evaluation\n",
        "            if i == 0 and epoch == 0: # Only print once per training run\n",
        "                src_batch_cpu = batch['input_ids']\n",
        "                tgt_batch_cpu = batch['labels']\n",
        "\n",
        "                print(f\"\\nBatch {i}: Inspecting U->E Val tensors before moving to {device}...\")\n",
        "                print(f\"Source batch shape: {src_batch_cpu.shape}, dtype: {src_batch_cpu.dtype}, device: {src_batch_cpu.device}\")\n",
        "                print(f\"Source batch min value: {src_batch_cpu.min().item()}, max value: {src_batch_cpu.max().item()}\")\n",
        "                print(f\"Source vocab size (Urdu input): {src_vocab_size}\")\n",
        "\n",
        "                print(f\"Target batch shape: {tgt_batch_cpu.shape}, dtype: {tgt_batch_cpu.dtype}, device: {tgt_batch_cpu.device}\")\n",
        "                print(f\"Target batch min value: {tgt_batch_cpu.min().item()}, max value: {tgt_batch_cpu.max().item()}\")\n",
        "                print(f\"Target vocab size (English labels): {tgt_vocab_size}\")\n",
        "                print(\"-----------------------------------------------------------------\")\n",
        "            # --- End diagnostic prints ---\n",
        "\n",
        "            src = batch['input_ids'].to(device)\n",
        "            tgt = batch['labels'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            labels = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            output = urdu_to_english_model(src, tgt_input)\n",
        "            output = output.view(-1, tgt_vocab_size)\n",
        "\n",
        "            loss = urdu_to_english_criterion(output, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(urdu_to_english_val_dataloader)\n",
        "    print(f\"Urdu->English Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nUrdu->English Training complete!\")\n",
        "\n",
        "\n",
        "# --- Step 7: Save the Trained Urdu->English Model ---\n",
        "print(f\"\\nSaving Urdu->English model state_dict to {urdu_to_english_model_save_path}...\")\n",
        "try:\n",
        "    torch.save(urdu_to_english_model.state_dict(), urdu_to_english_model_save_path)\n",
        "    print(\"Urdu->English model state_dict saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving Urdu->English model state_dict: {e}\")\n",
        "\n",
        "\n",
        "# --- Step 8: Prepare Test Set for Urdu->English BLEU Evaluation ---\n",
        "# Use the validation set indices from the U->E split as the test set for BLEU\n",
        "# In a real scenario, use a completely separate test set indices.\n",
        "urdu_to_english_test_indices_for_bleu = urdu_to_english_val_dataset.indices if hasattr(urdu_to_english_val_dataset, 'indices') else range(len(urdu_to_english_val_dataset))\n",
        "\n",
        "if len(urdu_to_english_test_indices_for_bleu) > 0:\n",
        "    print(f\"\\nUsing {len(urdu_to_english_test_indices_for_bleu)} samples from the original dataframe indices (from U->E validation split) as test set for BLEU evaluation.\")\n",
        "else:\n",
        "     print(\"\\nNot enough data to create a test set for Urdu->English BLEU evaluation.\")\n",
        "\n",
        "\n",
        "# --- Step 9: Calculate Urdu->English BLEU Score ---\n",
        "if len(urdu_to_english_test_indices_for_bleu) > 0:\n",
        "    print(\"\\nCalculating Corpus BLEU score on the Urdu->English test set...\")\n",
        "    urdu_to_english_model.eval() # Ensure the trained model is in evaluation mode\n",
        "\n",
        "    hypotheses = [] # Model's translated token lists (English)\n",
        "    references = [] # Actual target token lists (list of lists) (English)\n",
        "\n",
        "    print(\"Generating translations for Urdu->English BLEU evaluation...\")\n",
        "    for original_idx_in_df in tqdm(urdu_to_english_test_indices_for_bleu, desc=\"Translating U->E test set\"):\n",
        "        original_urdu_text = str(df.iloc[original_idx_in_df]['urdu']) # Get original Urdu text from dataframe\n",
        "        original_english_text = str(df.iloc[original_idx_in_df]['english']) # Get original English text (reference)\n",
        "\n",
        "        # Generate translation using the custom_translate_urdu_to_english function\n",
        "        translated_english = custom_translate_urdu_to_english(\n",
        "            original_urdu_text, urdu_to_english_model, tokenizer, device,\n",
        "            max_length=max_length,\n",
        "            start_token_id=start_token_id, # Start token for English\n",
        "            end_token_id=end_token_id,     # End token for English\n",
        "            pad_idx=pad_idx\n",
        "        )\n",
        "\n",
        "        # Tokenize both the hypothesis (translated English) and reference (original English)\n",
        "        hypothesis_tokens = tokenizer.encode_as_pieces(translated_english)\n",
        "        reference_tokens = [tokenizer.encode_as_pieces(original_english_text)] # Reference is original English\n",
        "\n",
        "        hypotheses.append(hypothesis_tokens)\n",
        "        references.append(reference_tokens)\n",
        "\n",
        "    # Calculate corpus BLEU score\n",
        "    if hypotheses and references and len(hypotheses) == len(references):\n",
        "        try:\n",
        "            bleu_score_urdu_to_english = corpus_bleu(references, hypotheses)\n",
        "            print(f\"\\nCorpus BLEU Score (Urdu->English): {bleu_score_urdu_to_english:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error calculating Urdu->English BLEU score: {e}\")\n",
        "             print(\"This might happen if there are empty sequences or other issues with tokenization.\")\n",
        "    else:\n",
        "        print(\"\\nCould not calculate Urdu->English BLEU score: Hypotheses or references list is empty or their lengths mismatch.\")\n",
        "        print(f\"Number of hypotheses: {len(hypotheses)}\")\n",
        "        print(f\"Number of references: {len(references)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo test dataset samples available for Urdu->English BLEU evaluation.\")\n",
        "\n",
        "print(\"\\nUrdu->English BLEU evaluation complete.\")"
      ],
      "metadata": {
        "id": "NLqv8PB5AKNk",
        "outputId": "f0814581-bb4d-4703-b9a3-276b14cb1c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_LAUNCH_BLOCKING is set to 1\n",
            "Using device: cuda\n",
            "\n",
            "--- Environment and Compatibility Checks ---\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Number of CUDA devices: 1\n",
            "CUDA version: 12.4\n",
            "Current CUDA device: 0\n",
            "GPU Name: Tesla T4\n",
            "GPU Capability: (7, 5)\n",
            "PyTorch built with CUDA: True\n",
            "\n",
            "--- Simple CUDA Tensor Test ---\n",
            "Error creating/moving test tensor to cuda: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Fundamental CUDA test failed. Your CUDA/PyTorch setup may be incompatible or corrupted.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-14-1246981847>\", line 59, in <cell line: 0>\n",
            "    test_tensor = torch.randn(10).to(device)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-14-1246981847>\", line 67, in <cell line: 0>\n",
            "    sys.exit()\n",
            "SystemExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1246981847>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Attempt to create and move a small tensor to the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Successfully created and moved a test tensor to {device}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-1246981847>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# We exit here because training requiring CUDA will fail anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: Running complex environment setup in a single cell might not work reliably.\n",
        "# This is only a last resort if dedicated terminal access is unavailable.\n",
        "\n",
        "# Try installing PyTorch and related libraries in the current Colab environment\n",
        "# Replace the command below with the exact one from the PyTorch website for your OS/Pip/CUDA 12.1 or 12.4\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install other libraries\n",
        "!pip install pandas openpyxl sentencepiece tqdm nltk\n",
        "\n",
        "# --- Minimal CUDA Test (within the same cell) ---\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"CUDA version reported by PyTorch: {torch.version.cuda}\")\n",
        "    try:\n",
        "        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error getting GPU details: {e}\")\n",
        "         print(\"This might indicate a deeper driver/CUDA toolkit issue.\")\n",
        "\n",
        "    print(\"\\n--- Attempting simple CUDA tensor test ---\")\n",
        "    try:\n",
        "        test_tensor = torch.randn(10).to(device)\n",
        "        print(\"Successfully created and moved a test tensor to CUDA.\")\n",
        "        print(test_tensor)\n",
        "        print(\"\\nBasic CUDA test PASSED.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error with CUDA tensor test: {e}\")\n",
        "        print(\"\\nBasic CUDA functionality FAILED.\")\n",
        "        print(\"This confirms an issue with your PyTorch/CUDA/Driver setup.\")\n",
        "        # sys.exit(1) # Avoid sys.exit in notebooks if you want to continue other cells\n",
        "\n",
        "else:\n",
        "    print(\"\\nCUDA is not available (PyTorch reports).\")\n",
        "    print(\"Basic CUDA test SKIPPED (no CUDA device found by PyTorch).\")\n",
        "    # sys.exit(1) # Avoid sys.exit in notebooks\n",
        "\n",
        "print(\"Script finished.\") # This line will only print if the CUDA test passes (or is skipped)\n"
      ],
      "metadata": {
        "id": "l-0C1fK5B8Kp",
        "outputId": "b70d7b6a-95a3-4227-da1c-f5e3d2131952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "53d7594636d0422fbc3278e26326f8a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Is CUDA available: True\n",
            "Using device: cuda\n",
            "CUDA version reported by PyTorch: 12.4\n",
            "Number of CUDA devices: 1\n",
            "GPU Name: Tesla T4\n",
            "\n",
            "--- Attempting simple CUDA tensor test ---\n",
            "Error with CUDA tensor test: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "Basic CUDA functionality FAILED.\n",
            "This confirms an issue with your PyTorch/CUDA/Driver setup.\n",
            "Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8YVA_634OFk"
      },
      "source": [
        "Welcome to this Colab where you will get a quick introduction to the Python programming language and the environment used for the course's exercises: Colab.\n",
        "\n",
        "Colab is a Python development environment that runs in the browser using Google Cloud.\n",
        "\n",
        "For example, to print \"Hello World\", just hover the mouse over [ ] and press the play button to the upper left. Or press shift-enter to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X9uIpOS2zx7k",
        "outputId": "4338435f-d78a-4df3-dc14-5e1f42df6e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello World\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "## Functions, Conditionals, and Iteration\n",
        "Let's create a Python function, and call it from a loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRllo2HLfXiu"
      },
      "outputs": [],
      "source": [
        "def HelloWorldXY(x, y):\n",
        "  if (x < 10):\n",
        "    print(\"Hello World, x was < 10\")\n",
        "  elif (x < 20):\n",
        "    print(\"Hello World, x was >= 10 but < 20\")\n",
        "  else:\n",
        "    print(\"Hello World, x was >= 20\")\n",
        "  return x + y\n",
        "\n",
        "for i in range(8, 25, 5):  # i=8, 13, 18, 23 (start, stop, step)\n",
        "  print(\"--- Now running with i: {}\".format(i))\n",
        "  r = HelloWorldXY(i,i)\n",
        "  print(\"Result from HelloWorld: {}\".format(r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHNmDCh0JpVP"
      },
      "outputs": [],
      "source": [
        "print(HelloWorldXY(1,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiZG7uhm8qCF"
      },
      "source": [
        "Easy, right?\n",
        "\n",
        "If you want a loop starting at 0 to 2 (exclusive) you could do any of the following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8YQN1H41L-Y"
      },
      "outputs": [],
      "source": [
        "print(\"Iterate over the items. `range(2)` is like a list [0,1].\")\n",
        "for i in range(2):\n",
        "  print(i)\n",
        "\n",
        "print(\"Iterate over an actual list.\")\n",
        "for i in [0,1]:\n",
        "  print(i)\n",
        "\n",
        "print(\"While works\")\n",
        "i = 0\n",
        "while i < 2:\n",
        "  print(i)\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIgmFZq4zszl"
      },
      "outputs": [],
      "source": [
        "print(\"Python supports standard key words like continue and break\")\n",
        "while True:\n",
        "  print(\"Entered while\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QyOUhFw1OUX"
      },
      "source": [
        "## Numpy and lists\n",
        "Python has lists built into the language.\n",
        "However, we will use a library called numpy for this.\n",
        "Numpy gives you lots of support functions that are useful when doing Machine Learning.\n",
        "\n",
        "Here, you will also see an import statement. This statement makes the entire numpy package available and we can access those symbols using the abbreviated 'np' syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dxk4q-jzEy4"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Make numpy available using np.\n",
        "\n",
        "# Create a numpy array, and append an element\n",
        "a = np.array([\"Hello\", \"World\"])\n",
        "a = np.append(a, \"!\")\n",
        "print(\"Current array: {}\".format(a))\n",
        "print(\"Printing each element\")\n",
        "for i in a:\n",
        "  print(i)\n",
        "\n",
        "print(\"\\nPrinting each element and their index\")\n",
        "for i,e in enumerate(a):\n",
        "  print(\"Index: {}, was: {}\".format(i, e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTa8_9G3LV03"
      },
      "outputs": [],
      "source": [
        "print(\"\\nShowing some basic math on arrays\")\n",
        "b = np.array([0,1,4,3,2])\n",
        "print(\"Max: {}\".format(np.max(b)))\n",
        "print(\"Average: {}\".format(np.average(b)))\n",
        "print(\"Max index: {}\".format(np.argmax(b)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YaGj5n4LW7P"
      },
      "outputs": [],
      "source": [
        "print(\"\\nYou can print the type of anything\")\n",
        "print(\"Type of b: {}, type of b[0]: {}\".format(type(b), type(b[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6ilVhi9LXn_"
      },
      "outputs": [],
      "source": [
        "print(\"\\nUse numpy to create a [3,3] dimension array with random number\")\n",
        "c = np.random.rand(3, 3)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_Q-DkFCLYGA"
      },
      "outputs": [],
      "source": [
        "print(\"\\nYou can print the dimensions of arrays\")\n",
        "print(\"Shape of a: {}\".format(a.shape))\n",
        "print(\"Shape of b: {}\".format(b.shape))\n",
        "print(\"Shape of c: {}\".format(c.shape))\n",
        "print(\"...Observe, Python uses both [0,1,2] and (0,1,2) to specify lists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Jk4dG91dvD"
      },
      "source": [
        "## Colab Specifics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0cGd8sHEmKi"
      },
      "source": [
        "Colab is a virtual machine you can access directly. To run commands at the VM's terminal, prefix the line with an exclamation point (!).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLkfhyzq0W2y"
      },
      "outputs": [],
      "source": [
        "print(\"\\nDoing $ls on filesystem\")\n",
        "!ls -l\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR2WTN1cOZ1n"
      },
      "outputs": [],
      "source": [
        "print(\"Install numpy\")  # Just for test, numpy is actually preinstalled in all Colab instances\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuWRpQdatAIU"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "Create a code cell underneath this text cell and add code to:\n",
        "\n",
        "\n",
        "*   List the path of the current directory (pwd)\n",
        "* Go to / (cd) and list the content (ls -l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU-cJbMCR61P"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!cd /\n",
        "!ls -l\n",
        "print(\"Hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5jv0ouFREV"
      },
      "source": [
        "All usage of Colab in this course is completely free or charge. Even GPU usage is provided free of charge for some hours of usage every day.\n",
        "\n",
        "**Using GPUs**\n",
        "* Many of the exercises in the course executes more quickly by using GPU runtime: Runtime | Change runtime type | Hardware accelerator | GPU\n",
        "\n",
        "**Some final words on Colab**\n",
        "*   You execute each cell in order, you can edit & re-execute cells if you want\n",
        "*   Sometimes, this could have unintended consequences. For example, if you add a dimension to an array and execute the cell multiple times, then the cells after may not work. If you encounter problem reset your environment:\n",
        "  *   Runtime -> Restart runtime... Resets your Python shell\n",
        "  *   Runtime -> Restart all runtimes... Will reset the Colab image, and get you back to a 100% clean environment\n",
        "* You can also clear the output in the Colab by doing: Edit -> Clear all outputs\n",
        "* Colabs in this course are loaded from GitHub. Save to your Google Drive if you want a copy with your code/output: File -> Save a copy in Drive...\n",
        "\n",
        "**Learn More**\n",
        "*   Check out [this](https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvwLbzbnKJ59NkZvQAW9wLbx&index=3) episode of #CodingTensorFlow, and don't forget to subscribe to the YouTube channel ;)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ff8585a34f646aea617c80bc5bbd404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed92c0c741194967b1a7573b195b1452",
              "IPY_MODEL_769afa8f5d13494eb4229cf7e53c10f7",
              "IPY_MODEL_56523066ff5848a696df46f2614665ee"
            ],
            "layout": "IPY_MODEL_6e563386ca18484f87a11c6a812d3698"
          }
        },
        "ed92c0c741194967b1a7573b195b1452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46de9ca86c454cd4bca33b77fa498b87",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e33f1bc330452c96f5b34c8503d4b0",
            "value": "Training:   0%"
          }
        },
        "769afa8f5d13494eb4229cf7e53c10f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4174d9857b44102851deff37d59d4fc",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4e6423ca6fa45e7a3e0ac6cebbd3dc1",
            "value": 0
          }
        },
        "56523066ff5848a696df46f2614665ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de746834e4a54b91938570b58dd03070",
            "placeholder": "​",
            "style": "IPY_MODEL_78d5b3a957fb41fa8d699b48e135e9a0",
            "value": " 0/1509 [00:00&lt;?, ?it/s]"
          }
        },
        "6e563386ca18484f87a11c6a812d3698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46de9ca86c454cd4bca33b77fa498b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e33f1bc330452c96f5b34c8503d4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4174d9857b44102851deff37d59d4fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e6423ca6fa45e7a3e0ac6cebbd3dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de746834e4a54b91938570b58dd03070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d5b3a957fb41fa8d699b48e135e9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f919ed42c5f743ca9fa697e0c5d01f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a066317a18fa4fa2a9c05ddfbce207db",
              "IPY_MODEL_b56e2adc9e8948deb3e14be6b1010952",
              "IPY_MODEL_7dd8588531cf4772b2ce116e091c9223"
            ],
            "layout": "IPY_MODEL_0405c27495e34b509d47c90d5582b00d"
          }
        },
        "a066317a18fa4fa2a9c05ddfbce207db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4844aae30632468f8bdf929aa06d6b13",
            "placeholder": "​",
            "style": "IPY_MODEL_557f7c1580e8400fa5076855cc16d825",
            "value": "Training:   0%"
          }
        },
        "b56e2adc9e8948deb3e14be6b1010952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae55dc256c9d4dd6a89a57ed1093c27a",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eb30d7a7bce44a6a01861c14863cd43",
            "value": 0
          }
        },
        "7dd8588531cf4772b2ce116e091c9223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_290763e167d145d28537c25393937ba8",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ac616c422f433aa73edb22b4cb60f7",
            "value": " 0/1509 [00:00&lt;?, ?it/s]"
          }
        },
        "0405c27495e34b509d47c90d5582b00d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4844aae30632468f8bdf929aa06d6b13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "557f7c1580e8400fa5076855cc16d825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae55dc256c9d4dd6a89a57ed1093c27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb30d7a7bce44a6a01861c14863cd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "290763e167d145d28537c25393937ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ac616c422f433aa73edb22b4cb60f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d44afe798c9438aa0c86bf3c6a0382b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_098a73a4c6a44d3a801b0a00e08c6bd8",
              "IPY_MODEL_926e075f53674b2baf222bb02d32a67c",
              "IPY_MODEL_d02c708a4d844511822e1f99636fbc8a"
            ],
            "layout": "IPY_MODEL_6baf15d978e54b9aa179d20c23cdeb00"
          }
        },
        "098a73a4c6a44d3a801b0a00e08c6bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b326cfd15de481a97e0f3e8b6932c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_11e2e260a435453cb1d94cbfefcb5cc6",
            "value": "Training:   0%"
          }
        },
        "926e075f53674b2baf222bb02d32a67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9769e6e9db734f53bfed187e39ac0154",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f604ad6c9e4441f90b0ae884a078af7",
            "value": 0
          }
        },
        "d02c708a4d844511822e1f99636fbc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe61cdd29fd4174aefb0e4ae5ad3344",
            "placeholder": "​",
            "style": "IPY_MODEL_392680ea9e6449a2bdc4ee22b29c1b99",
            "value": " 0/1509 [00:00&lt;?, ?it/s]"
          }
        },
        "6baf15d978e54b9aa179d20c23cdeb00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b326cfd15de481a97e0f3e8b6932c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11e2e260a435453cb1d94cbfefcb5cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9769e6e9db734f53bfed187e39ac0154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f604ad6c9e4441f90b0ae884a078af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fe61cdd29fd4174aefb0e4ae5ad3344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392680ea9e6449a2bdc4ee22b29c1b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "987c50d889ae4fc48510f6c0575f8f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_834f051db59640ed9b5608c60789fd61",
              "IPY_MODEL_adf884b7d71947c29dfd14e98a461d95",
              "IPY_MODEL_203270b7a85f4f76851d83fe960c180a"
            ],
            "layout": "IPY_MODEL_50da8dd7e7784301989b46e0ea94c73f"
          }
        },
        "834f051db59640ed9b5608c60789fd61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30e3b63d749e47cc8f67918b6ff886c4",
            "placeholder": "​",
            "style": "IPY_MODEL_86fdb2747daf4396ae58a172c34b7c1a",
            "value": "Training:   0%"
          }
        },
        "adf884b7d71947c29dfd14e98a461d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8e0e1a77bb4cf5b8c98559120ffade",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff96a82bec3343c0ae385a109e320094",
            "value": 0
          }
        },
        "203270b7a85f4f76851d83fe960c180a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6715d13ee444bc98dd068aa01385d88",
            "placeholder": "​",
            "style": "IPY_MODEL_6f3c4970e53347bf956554bd32f07b0f",
            "value": " 0/1509 [00:00&lt;?, ?it/s]"
          }
        },
        "50da8dd7e7784301989b46e0ea94c73f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30e3b63d749e47cc8f67918b6ff886c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86fdb2747daf4396ae58a172c34b7c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf8e0e1a77bb4cf5b8c98559120ffade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff96a82bec3343c0ae385a109e320094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6715d13ee444bc98dd068aa01385d88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3c4970e53347bf956554bd32f07b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dc1618cbc494812b44e0848cc64b9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2918392c1e1d4f7c9adebe3d65a71d48",
              "IPY_MODEL_f731459121e340cd87e698dd4bffe828",
              "IPY_MODEL_836373d595a343e796e7a7dc28b3eb51"
            ],
            "layout": "IPY_MODEL_df5e74d41b154937b780ebda32b94ffd"
          }
        },
        "2918392c1e1d4f7c9adebe3d65a71d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f3b62ebf7a46a68bb7a04d3ac45381",
            "placeholder": "​",
            "style": "IPY_MODEL_0d94168911604faab8d7c41ca0f248e8",
            "value": "Training: 100%"
          }
        },
        "f731459121e340cd87e698dd4bffe828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def9e7580ddc412b92372b2a0aff4422",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fba5c1bd1edf4f05a94b396cb3aaccb7",
            "value": 1509
          }
        },
        "836373d595a343e796e7a7dc28b3eb51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ff6d473ec6b4197acde919c5ca8d2ad",
            "placeholder": "​",
            "style": "IPY_MODEL_aea134d69ce1493493164d140d7b0cc4",
            "value": " 1509/1509 [07:45&lt;00:00,  3.96it/s]"
          }
        },
        "df5e74d41b154937b780ebda32b94ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f3b62ebf7a46a68bb7a04d3ac45381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d94168911604faab8d7c41ca0f248e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "def9e7580ddc412b92372b2a0aff4422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba5c1bd1edf4f05a94b396cb3aaccb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ff6d473ec6b4197acde919c5ca8d2ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea134d69ce1493493164d140d7b0cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8af0d9dc8d7e4e27b5f481a15ca8cc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be313ca942734640916930f90fae9a34",
              "IPY_MODEL_340d866c86ca4dd9b88d841321b8255c",
              "IPY_MODEL_c637893299634c078545dfc2ac71a5d0"
            ],
            "layout": "IPY_MODEL_cce50bbebd4448e49afaaf6b9a2f1bf2"
          }
        },
        "be313ca942734640916930f90fae9a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8401d5cf7f470aac55091e2b52c725",
            "placeholder": "​",
            "style": "IPY_MODEL_a840f49a2e794659978177e4ceb4e780",
            "value": "Validation: 100%"
          }
        },
        "340d866c86ca4dd9b88d841321b8255c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d27b161f8a9452892f2af067ea0c0e3",
            "max": 378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90e20ea0b543472f94a78ce3ded74afe",
            "value": 378
          }
        },
        "c637893299634c078545dfc2ac71a5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69bb1b9773ed4d4abc2a9e7c55174d43",
            "placeholder": "​",
            "style": "IPY_MODEL_7734e366880a4dff897f18859e2c1811",
            "value": " 378/378 [00:39&lt;00:00,  9.57it/s]"
          }
        },
        "cce50bbebd4448e49afaaf6b9a2f1bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c8401d5cf7f470aac55091e2b52c725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a840f49a2e794659978177e4ceb4e780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d27b161f8a9452892f2af067ea0c0e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e20ea0b543472f94a78ce3ded74afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69bb1b9773ed4d4abc2a9e7c55174d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7734e366880a4dff897f18859e2c1811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0a605fb22de4dee87104a6bcd172b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_302b64d1510543c7ba4a61ee08e1325b",
              "IPY_MODEL_3dff26c886814bf3be3367eae71a7f62",
              "IPY_MODEL_a22e928f77b84279b45ffff73bcb2c87"
            ],
            "layout": "IPY_MODEL_4f44254cc374403a9f688a968ab462d4"
          }
        },
        "302b64d1510543c7ba4a61ee08e1325b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1a81f0f7324094bc5c385f498ff5e4",
            "placeholder": "​",
            "style": "IPY_MODEL_be5fa683451d4788a088ada3a46f8157",
            "value": "Training: 100%"
          }
        },
        "3dff26c886814bf3be3367eae71a7f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2afcb8a53c5409eb81a25441993f493",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a78adae9c69346d6a2441c59b1e8e66a",
            "value": 1509
          }
        },
        "a22e928f77b84279b45ffff73bcb2c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e0d79ea58849caaee88a053e90dce3",
            "placeholder": "​",
            "style": "IPY_MODEL_b035738cbe71490c9f53ecf9489dc7fd",
            "value": " 1509/1509 [07:47&lt;00:00,  3.87it/s]"
          }
        },
        "4f44254cc374403a9f688a968ab462d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1a81f0f7324094bc5c385f498ff5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5fa683451d4788a088ada3a46f8157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2afcb8a53c5409eb81a25441993f493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78adae9c69346d6a2441c59b1e8e66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23e0d79ea58849caaee88a053e90dce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b035738cbe71490c9f53ecf9489dc7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90cb207d701c4e2eba2175102b0e2c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aadd5d5a786f4903878802567a67ace4",
              "IPY_MODEL_8154ca3deeb44079b8a8d30495fd2f87",
              "IPY_MODEL_38bba5d7e9e241c2b34d613e4dcd7072"
            ],
            "layout": "IPY_MODEL_01c57aeee2064016895006c244fe40e8"
          }
        },
        "aadd5d5a786f4903878802567a67ace4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1007719aa0944ed8ac2055e4df514fc0",
            "placeholder": "​",
            "style": "IPY_MODEL_600feadbcf8e4b608fd46e8e2b164f6f",
            "value": "Validation: 100%"
          }
        },
        "8154ca3deeb44079b8a8d30495fd2f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf62b8ea670d435cb9a7aaf0d2560ac3",
            "max": 378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf7843b3576440d19824c86063f1db7e",
            "value": 378
          }
        },
        "38bba5d7e9e241c2b34d613e4dcd7072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00256858cf2c4753a9a5ec86a8603e73",
            "placeholder": "​",
            "style": "IPY_MODEL_41df02db652d48e89b480fcb09bd22dd",
            "value": " 378/378 [00:39&lt;00:00,  8.86it/s]"
          }
        },
        "01c57aeee2064016895006c244fe40e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1007719aa0944ed8ac2055e4df514fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "600feadbcf8e4b608fd46e8e2b164f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf62b8ea670d435cb9a7aaf0d2560ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7843b3576440d19824c86063f1db7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00256858cf2c4753a9a5ec86a8603e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41df02db652d48e89b480fcb09bd22dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6868a4502d9948c793fe6df508b6bac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32b5aeb6b7f34215824b312ca4c35db9",
              "IPY_MODEL_eebf4b425ce24be09805b8027bb3624e",
              "IPY_MODEL_dec0339b13884592964c9808df31168d"
            ],
            "layout": "IPY_MODEL_4afb2196251c48e4853375e62f2c039a"
          }
        },
        "32b5aeb6b7f34215824b312ca4c35db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba262c4f55344b1684d4ca85aa048b0c",
            "placeholder": "​",
            "style": "IPY_MODEL_3b27ed5a2eae48aaaa9f6e9ffc6e0dc6",
            "value": "Training: 100%"
          }
        },
        "eebf4b425ce24be09805b8027bb3624e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93377cf1bf204017afa105884c9fc26f",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fce5d6ae10384bbcbc9a9daa7b2c43b1",
            "value": 1509
          }
        },
        "dec0339b13884592964c9808df31168d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b438e3d270f4d6ab24c2e908f9385fa",
            "placeholder": "​",
            "style": "IPY_MODEL_1efba444f0514c2e99edebdb2db99027",
            "value": " 1509/1509 [07:47&lt;00:00,  3.94it/s]"
          }
        },
        "4afb2196251c48e4853375e62f2c039a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba262c4f55344b1684d4ca85aa048b0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b27ed5a2eae48aaaa9f6e9ffc6e0dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93377cf1bf204017afa105884c9fc26f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce5d6ae10384bbcbc9a9daa7b2c43b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b438e3d270f4d6ab24c2e908f9385fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1efba444f0514c2e99edebdb2db99027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ce73824499f48249dd2e2b59c4c6118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe8f153337cb4ebc8cd8be08f067c70b",
              "IPY_MODEL_c28ef18d2b8d4f79b2535fce17580d84",
              "IPY_MODEL_a5bfc15d177e408fbb6557aa4c6917a8"
            ],
            "layout": "IPY_MODEL_82dc7a0ab4a34550ba1459abc3afd1a4"
          }
        },
        "fe8f153337cb4ebc8cd8be08f067c70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bac2152886184a2d8d18b3dcef2b3f74",
            "placeholder": "​",
            "style": "IPY_MODEL_0396ed34c5d0436688756d42835330ca",
            "value": "Validation: 100%"
          }
        },
        "c28ef18d2b8d4f79b2535fce17580d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c423f1dc854d8aa12e38d74ba0b381",
            "max": 378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb064bda3cce4b5cbb32541976956753",
            "value": 378
          }
        },
        "a5bfc15d177e408fbb6557aa4c6917a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62837827b61541f98aa2f49553a5508e",
            "placeholder": "​",
            "style": "IPY_MODEL_5003c9098ebf4bc49f116e913fba31c1",
            "value": " 378/378 [00:39&lt;00:00,  9.79it/s]"
          }
        },
        "82dc7a0ab4a34550ba1459abc3afd1a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac2152886184a2d8d18b3dcef2b3f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0396ed34c5d0436688756d42835330ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61c423f1dc854d8aa12e38d74ba0b381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb064bda3cce4b5cbb32541976956753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62837827b61541f98aa2f49553a5508e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5003c9098ebf4bc49f116e913fba31c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb8bfd0a459444078eaaa12f06021128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_103de46aa1834bb7a83c990f33e78a35",
              "IPY_MODEL_a975be5699bd4ad683b674d118c0c2db",
              "IPY_MODEL_ea1b4cc0b4784333ae0abf858cea80b1"
            ],
            "layout": "IPY_MODEL_673d3ade7be84262acbdaa95966798c9"
          }
        },
        "103de46aa1834bb7a83c990f33e78a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28f7769cec3b4cb0acfeb239ce6cd317",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e772430bda49b789964e30d36e464b",
            "value": "Training: 100%"
          }
        },
        "a975be5699bd4ad683b674d118c0c2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ea2cef607343419158313a955d3d97",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_283966b971794a2382636c7c9f1cf72d",
            "value": 1509
          }
        },
        "ea1b4cc0b4784333ae0abf858cea80b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af6bd29626fc4b6c8ea71d50c8edac56",
            "placeholder": "​",
            "style": "IPY_MODEL_0a81e8e3c6ac412bb05517bf5d9808b9",
            "value": " 1509/1509 [07:46&lt;00:00,  4.01it/s]"
          }
        },
        "673d3ade7be84262acbdaa95966798c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28f7769cec3b4cb0acfeb239ce6cd317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0e772430bda49b789964e30d36e464b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1ea2cef607343419158313a955d3d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283966b971794a2382636c7c9f1cf72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af6bd29626fc4b6c8ea71d50c8edac56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a81e8e3c6ac412bb05517bf5d9808b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f64c4fd72234c7d84d61fdf4cefe0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_528eb77bd40d452ebb3f2f236f8f3f36",
              "IPY_MODEL_565ff793fa4b4a4f9934fa9b4c6c8a3e",
              "IPY_MODEL_2808f9678557484380903aa0a64ab4da"
            ],
            "layout": "IPY_MODEL_4cd2ba54a0a7429792d3dee6423cfc56"
          }
        },
        "528eb77bd40d452ebb3f2f236f8f3f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36b45f71f4bd46b7bb3277fbd571637f",
            "placeholder": "​",
            "style": "IPY_MODEL_b44f3149fc134fea9c23af728fd0e833",
            "value": "Validation: 100%"
          }
        },
        "565ff793fa4b4a4f9934fa9b4c6c8a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf11d4509d094743bca664c6d088a6ca",
            "max": 378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df3c68b510e14fc8ac9760af119669dc",
            "value": 378
          }
        },
        "2808f9678557484380903aa0a64ab4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2849ca7be44fd6823aeb95084f44e9",
            "placeholder": "​",
            "style": "IPY_MODEL_19fca84c1da047f3818ef8b7e1e0308c",
            "value": " 378/378 [00:38&lt;00:00,  9.88it/s]"
          }
        },
        "4cd2ba54a0a7429792d3dee6423cfc56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b45f71f4bd46b7bb3277fbd571637f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44f3149fc134fea9c23af728fd0e833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf11d4509d094743bca664c6d088a6ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3c68b510e14fc8ac9760af119669dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2849ca7be44fd6823aeb95084f44e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19fca84c1da047f3818ef8b7e1e0308c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2899b8631ed543748adaf835299d57fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c152e277c4fd43b3897ed90aef66072b",
              "IPY_MODEL_8d8d01e3fc404b779be251ea7c6fb5d0",
              "IPY_MODEL_bff1f6cba344461083bf91b1e76ff363"
            ],
            "layout": "IPY_MODEL_893ef115ae5a4a79a1add7630b42ba83"
          }
        },
        "c152e277c4fd43b3897ed90aef66072b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65372139a1d04094b526b327a6193da8",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ca6810cc8e4d22a3a2f5239c7b3444",
            "value": "Training: 100%"
          }
        },
        "8d8d01e3fc404b779be251ea7c6fb5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d0730b14b846b8b5796175923bd33c",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_391ff6f42fcd4eb4b4aeb5d89b39c214",
            "value": 1509
          }
        },
        "bff1f6cba344461083bf91b1e76ff363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ab5d973e4f4b0080ee3a3639ff88f6",
            "placeholder": "​",
            "style": "IPY_MODEL_d069c07fde104fafa9572e0835cb6460",
            "value": " 1509/1509 [07:46&lt;00:00,  3.95it/s]"
          }
        },
        "893ef115ae5a4a79a1add7630b42ba83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65372139a1d04094b526b327a6193da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5ca6810cc8e4d22a3a2f5239c7b3444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00d0730b14b846b8b5796175923bd33c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391ff6f42fcd4eb4b4aeb5d89b39c214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12ab5d973e4f4b0080ee3a3639ff88f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d069c07fde104fafa9572e0835cb6460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee6703279eb14dd5b18c0737f41c9431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c5f726e1ad940c9bcfff8b02669c410",
              "IPY_MODEL_82ddd28e28b348a2b62d4924a1d229d8",
              "IPY_MODEL_8854641c248c4e28a5ed62c507d6b357"
            ],
            "layout": "IPY_MODEL_f3d92ae15ab346c8b189242076508e71"
          }
        },
        "8c5f726e1ad940c9bcfff8b02669c410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e310da46172940ecb1d0df02c20057b8",
            "placeholder": "​",
            "style": "IPY_MODEL_5d5f70edd1954d2e97eb7e4a894c9129",
            "value": "Validation: 100%"
          }
        },
        "82ddd28e28b348a2b62d4924a1d229d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5332cbc38b2e4697a652310d22df8105",
            "max": 378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee81a0cb0028435d918aed70e06667f6",
            "value": 378
          }
        },
        "8854641c248c4e28a5ed62c507d6b357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57db203fc5c74ccab8fb14952c088db9",
            "placeholder": "​",
            "style": "IPY_MODEL_68f8ffe7f93645faadfdaae77f26eb43",
            "value": " 378/378 [00:39&lt;00:00,  9.45it/s]"
          }
        },
        "f3d92ae15ab346c8b189242076508e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e310da46172940ecb1d0df02c20057b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5f70edd1954d2e97eb7e4a894c9129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5332cbc38b2e4697a652310d22df8105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee81a0cb0028435d918aed70e06667f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57db203fc5c74ccab8fb14952c088db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f8ffe7f93645faadfdaae77f26eb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce4843d59d764efa90573c9a1f5fe82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2693647163ec445a8812d5cff3d9092e",
              "IPY_MODEL_fd7fa77e42b2436ab46a3eb4f349f38e",
              "IPY_MODEL_cdb06675daa04f2abef586e4a54849a7"
            ],
            "layout": "IPY_MODEL_78fa98d39c754818aac9f8948e757da1"
          }
        },
        "2693647163ec445a8812d5cff3d9092e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614d3a0007ff417fbb334f53f0ef44d5",
            "placeholder": "​",
            "style": "IPY_MODEL_9d74bf6f64c74a68827fb0d7b4aa3b42",
            "value": "Translating test set: 100%"
          }
        },
        "fd7fa77e42b2436ab46a3eb4f349f38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b1546d9d5df42a6b7ceb262ca2cf999",
            "max": 6033,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8d2bcbd24464f9da55d62f681600e9f",
            "value": 6033
          }
        },
        "cdb06675daa04f2abef586e4a54849a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c771cf57a6545258437cd1048bb0386",
            "placeholder": "​",
            "style": "IPY_MODEL_b7ae8f42c64d44709bff4ea67df0fc86",
            "value": " 6033/6033 [39:54&lt;00:00,  4.94it/s]"
          }
        },
        "78fa98d39c754818aac9f8948e757da1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "614d3a0007ff417fbb334f53f0ef44d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d74bf6f64c74a68827fb0d7b4aa3b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b1546d9d5df42a6b7ceb262ca2cf999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d2bcbd24464f9da55d62f681600e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c771cf57a6545258437cd1048bb0386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ae8f42c64d44709bff4ea67df0fc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3fb88ce58c74875b8333583eb24b002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ead40265b53e4313bfc45a04eae0a812",
              "IPY_MODEL_ba0f2eb7e4914a55a8aa215c4998834a",
              "IPY_MODEL_08c3259fb2644d8890f8d7d91d5bda46"
            ],
            "layout": "IPY_MODEL_57298a8c070148cfbd487e7cef8740ca"
          }
        },
        "ead40265b53e4313bfc45a04eae0a812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f33749d6ea44494b923bd8c73c528ec",
            "placeholder": "​",
            "style": "IPY_MODEL_753fe44ddb8f45fb814a7596c883baca",
            "value": "Urdu-&gt;English Training:   0%"
          }
        },
        "ba0f2eb7e4914a55a8aa215c4998834a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cab4ec0635d94910b71097abb8c7ea22",
            "max": 1509,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60c5a44fd5364a0fa5448bfa35a86800",
            "value": 0
          }
        },
        "08c3259fb2644d8890f8d7d91d5bda46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d23a2c942f9e4af58f613bf810e6b3d5",
            "placeholder": "​",
            "style": "IPY_MODEL_762b7fefd5164ed7b84e7d96969aa6a7",
            "value": " 0/1509 [00:00&lt;?, ?it/s]"
          }
        },
        "57298a8c070148cfbd487e7cef8740ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f33749d6ea44494b923bd8c73c528ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "753fe44ddb8f45fb814a7596c883baca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cab4ec0635d94910b71097abb8c7ea22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60c5a44fd5364a0fa5448bfa35a86800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d23a2c942f9e4af58f613bf810e6b3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762b7fefd5164ed7b84e7d96969aa6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}